{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Retrieval of Game Week Wise Player Match Statistics (2024-2025) from GitHub URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "owner = \"olbauday\"\n",
    "repo = \"FPL-Elo-Insights\"\n",
    "branch = \"main\"\n",
    "\n",
    "load_dotenv(dotenv_path=\"C:/PROJECT/.env\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise RuntimeError(\"Personal Access Token is not set\")\n",
    "    \n",
    "pms_path_24 = \"data/2024-2025/playermatchstats\"\n",
    "pms_save_folder_24 = r\"C:\\exp1\\Player Gameweek Stats 24\"\n",
    "players_path_24 = \"data/2024-2025/players\"\n",
    "matches_path_24 = \"data/2024-2025/matches\"\n",
    "players_save_folder_24 = r\"C:\\exp1\\Player Stats 24\"\n",
    "matches_save_folder_24 = r\"C:\\exp1\\Match Stats 24\"\n",
    "\n",
    "path_25 = \"data/2025-2026/By Tournament/Premier League\"\n",
    "save_folder_25 = r\"C:\\\\exp1\\\\\"\n",
    "file_dir_map = {\n",
    "    \"playermatchstats.csv\": os.path.join(save_folder_25, \"Player Gameweek Stats 25\"),\n",
    "    \"players.csv\": os.path.join(save_folder_25, \"Player Stats 25\"),\n",
    "    \"matches.csv\": os.path.join(save_folder_25, \"Match Stats 25\")\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms_df_24 = None\n",
    "players_df_24 = None\n",
    "matches_df_24 = None\n",
    "teams_df_24 = None\n",
    "pms_df_25 = None\n",
    "players_df_25 = None\n",
    "matches_df_25 = None\n",
    "teams_df_25 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadGitHubData(owner, repo, branch, base_path, local_save_folder):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{base_path}?ref={branch}\"\n",
    "    print(f\"Retrieving directory contents from GitHub: {base_path}\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Could not retrieve contents for {base_path}. Status code: {response.status_code}\")\n",
    "        print(\"Please check the Owner / Repository / Branch / Base Path.\")\n",
    "        return  \n",
    "    try:\n",
    "        contents = response.json()\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        print(\"Error: Failed to decode JSON response\")\n",
    "        return\n",
    "    for item in contents:\n",
    "        local_path = os.path.join(local_save_folder, item['name'])\n",
    "        if item['type'] == 'dir' and item['name'].startswith('GW'):\n",
    "            dir_exists = os.path.isdir(local_path)\n",
    "            if dir_exists:\n",
    "                print(f\"{item['name']} exists.\")\n",
    "            else:\n",
    "                print(f\"{item['name']} created.\")\n",
    "            new_path = f\"{base_path}/{item['name']}\"\n",
    "            downloadGitHubData(owner, repo, branch, new_path, local_path)\n",
    "        elif item['type'] == 'file' and item['name'].endswith('.csv'):\n",
    "            download_url = item.get('download_url')\n",
    "            if not download_url:\n",
    "                print(f\"No URL found for downloading {item['name']}\")\n",
    "                continue\n",
    "            if os.path.exists(local_path):\n",
    "                # print(f\"(SKIP) File already exists: {item['name']}\")\n",
    "                continue            \n",
    "            print(f\"Downloading: {item['name']}\")\n",
    "            file_dir = os.path.dirname(local_path)\n",
    "            os.makedirs(file_dir, exist_ok=True)\n",
    "            file_content_response = requests.get(download_url)\n",
    "            if file_content_response.status_code == 200:\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(file_content_response.content)\n",
    "                print(f\"{item['name']} has been saved successfully.\")\n",
    "            else:\n",
    "                print(f\"Downloading {item['name']}\")\n",
    "                print(f\"Status code: {file_content_response.status_code}\")\n",
    "    print(f\"Finished processing contents of: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadGitHubData_25(owner, repo, branch, base_path, local_save_folder):\n",
    "    \n",
    "    # define headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{base_path}?ref={branch}\"\n",
    "    \n",
    "    # generating response and handling reply\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Could not retrieve contents for {base_path}. Status code: {response.status_code}\")\n",
    "        print(\"Please check the Owner / Repository / Branch / Base Path.\")\n",
    "        return \n",
    "    try:\n",
    "        # storing the response in a variable\n",
    "        contents = response.json()\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        print(\"Error: Failed to decode JSON response\")\n",
    "        return\n",
    "    for item in contents:\n",
    "        item_name = item['name']\n",
    "        if item['type'] == 'dir' and item_name.startswith('GW'):\n",
    "            try:\n",
    "                gw_number = int(item_name[2:])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if not (17 <= gw_number <= 21):\n",
    "                print(f\"SKIP: {item_name}\")\n",
    "                continue\n",
    "            print(f\"Entering directory: {item_name}\")\n",
    "            new_path = f\"{base_path}/{item_name}\"\n",
    "            downloadGitHubData_25(owner, repo, branch, new_path, item_name) \n",
    "        elif item['type'] == 'file' and item_name.endswith('.csv'):\n",
    "            if item_name not in file_dir_map:\n",
    "                print(f\"SKIP: {item_name}\")\n",
    "                continue\n",
    "            gw_folder_name = local_save_folder\n",
    "            target_local_root = file_dir_map[item_name]\n",
    "            local_path = os.path.join(target_local_root, gw_folder_name, item_name)\n",
    "            download_url = item.get('download_url')\n",
    "            if not download_url:\n",
    "                print(f\"No url found for downloading {item_name}\")\n",
    "                continue\n",
    "            if os.path.exists(local_path):\n",
    "                print(f\"{item_name} exists. Skipping.\")\n",
    "                continue           \n",
    "            print(f\"Downloading file: {item_name} to {local_path}\")\n",
    "            file_dir = os.path.dirname(local_path)\n",
    "            os.makedirs(file_dir, exist_ok=True)\n",
    "            file_content_response = requests.get(download_url)\n",
    "            if file_content_response.status_code == 200:\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(file_content_response.content)\n",
    "                print(f\"{item_name} has been saved successfully.\")\n",
    "            else:\n",
    "                print(f\"Error downloading {item_name}. Status code: {file_content_response.status_code}\")\n",
    "    print(f\"Finished processing contents of: {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Loading Players Match, Players, Match Stats (2024-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 1. Players Match Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloadGitHubData(owner, repo, branch, pms_path_24, pms_save_folder_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"player-match-data-24\")\n",
    "if pms_df_24 is not None:\n",
    "    print(f\"The final combined dataframe already exists with {pms_df_24.shape[0]} rows and {pms_df_24.shape[1]} columns\")\n",
    "else:\n",
    "    all_gw_data = []\n",
    "    for idx in range(1, 39):\n",
    "        gw_folder = f\"GW{idx}\"\n",
    "        csv_file = os.path.join(root, gw_folder, \"playermatchstats.csv\")\n",
    "        if os.path.exists(csv_file):\n",
    "            print(f\"Loading Player Data from: {gw_folder}\")\n",
    "            temp_df = pd.read_csv(csv_file)\n",
    "            temp_df['Game Week'] = idx\n",
    "            all_gw_data.append(temp_df)\n",
    "        else: print(f\"File not found for: {gw_folder}\")\n",
    "    if all_gw_data:\n",
    "        pms_df_24 = pd.concat(all_gw_data, ignore_index=True)\n",
    "        print(f\"Succesfully combined all Game Week's Players Data into One single Data frame!\")\n",
    "        print(f\"Shape of the dataframe = {pms_df_24.shape}\")\n",
    "    else:\n",
    "        print(\"No files were found or loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2. Players Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloadGitHubData(owner, repo, branch, players_path_24, players_save_folder_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"player-data-24\")\n",
    "if players_df_24 is not None:\n",
    "    print(f\"The dataframe already exists with {players_df_24.shape[0]} rows and {players_df_24.shape[1]} columns\")\n",
    "else:\n",
    "    csv_file = os.path.join(root, \"players.csv\")\n",
    "    if os.path.exists(csv_file):\n",
    "        print(\"Loading Players Data\")\n",
    "        players_df_24 = pd.read_csv(csv_file)\n",
    "        print(\"Loaded successfully\")\n",
    "        print(f\"Shape of the dataframe: {players_df_24.shape}\")\n",
    "    else: print(f\"File not found at: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 3. Match Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloadGitHubData(owner, repo, branch, matches_path_24, matches_save_folder_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"match-data-github-24\")\n",
    "if matches_df_24 is not None:\n",
    "    print(f\"The dataframe already exists with {matches_df_24.shape[0]} rows and {matches_df_24.shape[1]} columns\")\n",
    "else:\n",
    "    csv_file = os.path.join(root, \"matches.csv\")\n",
    "    if os.path.exists(csv_file):\n",
    "        print(\"Loading Matches Data\")\n",
    "        matches_df_24 = pd.read_csv(csv_file)\n",
    "        print(\"Loaded successfully\")\n",
    "        print(f\"Shape of the dataframe: {matches_df_24.shape}\")\n",
    "    else: print(f\"File not found at: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df_24['kickoff_time'] = pd.to_datetime(matches_df_24['kickoff_time'], format=\"mixed\")\n",
    "matches_df_24['gameweek'] = matches_df_24['gameweek'].astype(int)\n",
    "matches_df_24 = matches_df_24.sort_values(by=['gameweek', 'kickoff_time'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 4. Teams Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"team-data-24\")\n",
    "if teams_df_24 is not None:\n",
    "    print(f\"The dataframe already exists with {teams_df_24.shape[0]} rows and {teams_df_24.shape[1]} columns\")\n",
    "else:\n",
    "    csv_file = os.path.join(root, \"teams24.csv\")\n",
    "    if os.path.exists(csv_file):\n",
    "        print(\"Loading Teams Data\")\n",
    "        teams_df_24 = pd.read_csv(csv_file)\n",
    "        print(\"Loaded successfully\")\n",
    "        print(f\"Shape of the dataframe: {teams_df_24.shape}\")\n",
    "    else: print(f\"File not found at: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify and validate\n",
    "print(f\"Player Match Statistics: {pms_df_24.shape}\")\n",
    "print(f\"Player Statistics: {players_df_24.shape}\")\n",
    "print(f\"Match Statistics: {matches_df_24.shape}\")\n",
    "print(f\"Team Statistics: {teams_df_24.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Loading Players Match, Players, Match Stats, Team Stats (2025-2026)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 1. Players Match Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadGitHubData_25(owner, repo, branch, path_25, save_folder_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"player-match-data-25\")\n",
    "if pms_df_25 is not None:\n",
    "    print(f\"The final combined dataframe already exists with {pms_df_25.shape[0]} rows and {pms_df_25.shape[1]} columns\")\n",
    "else:\n",
    "    all_gw_data = []\n",
    "    for idx in range(1, 7):\n",
    "        gw_folder = f\"GW{idx}\"\n",
    "        csv_file = os.path.join(root, gw_folder, \"playermatchstats.csv\")\n",
    "        if os.path.exists(csv_file):\n",
    "            print(f\"Loading Player Data from: {gw_folder}\")\n",
    "            temp_df = pd.read_csv(csv_file)\n",
    "            temp_df['Game Week'] = idx\n",
    "            all_gw_data.append(temp_df)\n",
    "        else: print(f\"File not found for: {gw_folder}\")\n",
    "    if all_gw_data:\n",
    "        pms_df_25 = pd.concat(all_gw_data, ignore_index=True)\n",
    "        print(f\"Succesfully combined all Game Week's Players Data into One single Data frame!\")\n",
    "        print(f\"Shape of the dataframe = {pms_df_25.shape}\")\n",
    "    else:\n",
    "        print(\"No files were found or loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2. Players Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"player-data-25\")\n",
    "if players_df_25 is not None:\n",
    "    print(f\"The final combined dataframe already exists with {players_df_25.shape[0]} rows and {players_df_25.shape[1]} columns\")\n",
    "else:\n",
    "    all_gw_data = []\n",
    "    for idx in range(1, 7):\n",
    "        gw_folder = f\"GW{idx}\"\n",
    "        csv_file = os.path.join(root, gw_folder, \"players.csv\")\n",
    "        if os.path.exists(csv_file):\n",
    "            print(f\"Loading Player Data from: {gw_folder}\")\n",
    "            temp_df_25 = pd.read_csv(csv_file)\n",
    "            temp_df_25['Game Week'] = idx\n",
    "            all_gw_data.append(temp_df_25)\n",
    "        else: print(f\"File not found for: {gw_folder}\")\n",
    "    if all_gw_data:\n",
    "        players_df_25 = pd.concat(all_gw_data, ignore_index=True)\n",
    "        print(f\"Succesfully combined all Players Data into One single Data frame!\")\n",
    "        print(f\"Shape of the dataframe = {players_df_25.shape}\")\n",
    "    else:\n",
    "        print(\"No files were found or loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3. Match Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"match-data-github-25\")\n",
    "if matches_df_25 is not None:\n",
    "    print(f\"The final combined dataframe already exists with {matches_df_25.shape[0]} rows and {matches_df_25.shape[1]} columns\")\n",
    "else:\n",
    "    all_gw_data = []\n",
    "    for idx in range(1, 7):\n",
    "        gw_folder = f\"GW{idx}\"\n",
    "        csv_file = os.path.join(root, gw_folder, \"matches.csv\")\n",
    "        if os.path.exists(csv_file):\n",
    "            print(f\"Loading Match Data from: {gw_folder}\")\n",
    "            temp_df_25 = pd.read_csv(csv_file)\n",
    "            temp_df_25['Game Week'] = idx\n",
    "            all_gw_data.append(temp_df_25)\n",
    "        else: print(f\"File not found for: {gw_folder}\")\n",
    "    if all_gw_data:\n",
    "        matches_df_25 = pd.concat(all_gw_data, ignore_index=True)\n",
    "        print(f\"Succesfully combined all Matches Data into One single Data frame!\")\n",
    "        print(f\"Shape of the dataframe = {matches_df_25.shape}\")\n",
    "    else:\n",
    "        print(\"No files were found or loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df_25['kickoff_time'] = pd.to_datetime(matches_df_25['kickoff_time'], format=\"mixed\")\n",
    "matches_df_25['gameweek'] = matches_df_25['gameweek'].astype(int)\n",
    "matches_df_25 = matches_df_25.sort_values(by=['gameweek', 'kickoff_time'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 4. Teams Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(\"C:\\\\PROJECT\\\\data\\\\raw-data\", \"team-data-25\")\n",
    "if teams_df_25 is not None:\n",
    "    print(f\"The dataframe already exists with {teams_df_25.shape[0]} rows and {teams_df_25.shape[1]} columns\")\n",
    "else:\n",
    "    csv_file = os.path.join(root, \"teams25.csv\")\n",
    "    if os.path.exists(csv_file):\n",
    "        print(\"Loading Teams Data\")\n",
    "        teams_df_25 = pd.read_csv(csv_file)\n",
    "        print(\"Loaded successfully\")\n",
    "        print(f\"Shape of the dataframe: {teams_df_25.shape}\")\n",
    "    else: print(f\"File not found at: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Dimensionality check of all dataframes gathered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "1. 2024-2025 Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Player Match Statistics: {pms_df_24.shape}\")\n",
    "print(f\"Player Statistics: {players_df_24.shape}\")\n",
    "print(f\"Match Statistics: {matches_df_24.shape}\")\n",
    "print(f\"Team Statistics: {teams_df_24.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "2. 2025-2026 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Player Match Statistics: {pms_df_25.shape}\")\n",
    "print(f\"Player Statistics: {players_df_25.shape}\")\n",
    "print(f\"Match Statistics: {matches_df_25.shape}\")\n",
    "print(f\"Team Statistics: {teams_df_25.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Data Merging & Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### I. Players Match Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms_24_columns = set(pms_df_24.columns)\n",
    "pms_25_columns = set(pms_df_25.columns)\n",
    "pms_different_columns = pms_25_columns.difference(pms_24_columns)\n",
    "combined_pms = pd.concat([pms_df_24, pms_df_25], ignore_index=True)\n",
    "print(f\"Shape of combined PMS data: {combined_pms.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### II. Players Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_24_columns = set(players_df_24.columns)\n",
    "players_25_columns = set(players_df_25.columns)\n",
    "players_different_columns = players_25_columns.difference(players_24_columns)\n",
    "combined_players = pd.concat([players_df_24, players_df_25], ignore_index=True).drop(columns=players_different_columns, errors='ignore')\n",
    "combined_players = combined_players.drop_duplicates(subset=['player_id'], keep='first')\n",
    "print(f\"Shape of combined Players data: {combined_players.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### III. Matches Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_24_columns = set(matches_df_24.columns)\n",
    "matches_25_columns = set(matches_df_25.columns)\n",
    "matches_different_columns = matches_25_columns.difference(matches_24_columns)\n",
    "matches_df_24['season'] = 2024\n",
    "matches_df_25['season'] = 2025\n",
    "combined_matches = pd.concat([matches_df_24, matches_df_25], ignore_index=True).drop(columns=matches_different_columns, errors='ignore')\n",
    "print(f\"Shape of combined Matches data: {combined_matches.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### IV. Teams Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_df_25 = teams_df_25.sort_values(by='id', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_stats = pd.concat([teams_df_24, teams_df_25], ignore_index=True).drop(columns=['fotmob_name'], errors='ignore').drop_duplicates()\n",
    "teams_stats.loc[teams_stats.index[:20], 'season'] = 2024\n",
    "teams_stats.loc[teams_stats.index[20:], 'season'] = 2025\n",
    "teams_stats['season'] = teams_stats['season'].astype(int)\n",
    "print(f\"Shape of Overall Teams data: {teams_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Working with the PMS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### It is evident that the features: 'player_id' & 'match_id' are the foreign keys and refer to the Players data and Match data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pms_columns = set(combined_pms.columns)\n",
    "combined_players_columns = set(combined_players.columns)\n",
    "pms_players_same = combined_pms_columns.intersection(combined_players_columns)\n",
    "print(f\"Same columns from PMS and Players data: {pms_players_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms_players = combined_pms.merge(\n",
    "    combined_players[['player_id', 'position', 'team_code']],\n",
    "    on='player_id',\n",
    "    how='left',\n",
    "    suffixes=('_pms', '_static')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of combined PMS data: {combined_pms.shape}\")\n",
    "print(f\"Shape of combined Players data: {combined_players.shape}\")\n",
    "print(f\"Shape after merging PMS and Players data: {pms_players.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Divide the dataset on the basis of the positions of different players "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms_players['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_map = {\n",
    "    'Goalkeeper':'GK',\n",
    "    'Defender':'DEF',\n",
    "    'Midfielder':'MID',\n",
    "    'Forward':'FWD',\n",
    "    'Unknown':'NA'\n",
    "}\n",
    "pms_players['position_group'] = pms_players['position'].map(position_map)\n",
    "pms_players_gk = pms_players[pms_players['position_group']=='GK'].copy()\n",
    "pms_players_def = pms_players[pms_players['position_group']=='DEF'].copy()\n",
    "pms_players_mid = pms_players[pms_players['position_group']=='MID'].copy()\n",
    "pms_players_fwd = pms_players[pms_players['position_group']=='FWD'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goalkeeper stats\n",
    "gk_stats = pms_players_gk[['player_id', 'team_code', 'match_id', 'Game Week', 'minutes_played', \n",
    "                           'gk_accurate_passes', 'gk_accurate_long_balls', \n",
    "                           'saves', 'saves_inside_box', \n",
    "                           'goals_conceded', 'team_goals_conceded',\n",
    "                           'xgot_faced', 'goals_prevented',\n",
    "                           'sweeper_actions', 'high_claim']].copy()\n",
    "\n",
    "# defender stats\n",
    "def_stats = pms_players_def[['player_id', 'match_id', 'team_code', 'Game Week', 'minutes_played', 'xg', 'xa',\n",
    "                            'accurate_passes', 'accurate_long_balls', 'final_third_passes',\n",
    "                            'tackles_won', 'interceptions', 'recoveries', 'blocks', 'clearances', \n",
    "                            'headed_clearances', 'dribbled_past', 'duels_won',\n",
    "                            'ground_duels_won', 'aerial_duels_won', 'was_fouled', 'fouls_committed',\n",
    "                            'tackles', 'distance_covered', 'defensive_contributions']].copy()\n",
    "def_stats['tackles_won_percentage'] = def_stats['tackles_won'] / def_stats['tackles']\n",
    "def_stats = def_stats.drop(columns='tackles', errors='ignore')\n",
    "\n",
    "# midfielder stats\n",
    "mid_stats = pms_players_mid[['player_id', 'match_id', 'team_code', 'Game Week', 'minutes_played',\n",
    "                             'goals', 'assists', 'xg', 'xa',\n",
    "                             'accurate_passes', 'accurate_crosses', 'accurate_long_balls', 'final_third_passes',\n",
    "                             'total_shots', 'shots_on_target',\n",
    "                             'chances_created', 'touches',\n",
    "                             'successful_dribbles', 'corners',\n",
    "                             'penalties_scored', 'penalties_missed',\n",
    "                             'tackles_won', 'interceptions', 'recoveries', 'blocks', 'clearances',\n",
    "                             'dribbled_past', 'duels_won', 'ground_duels_won', 'aerial_duels_won',\n",
    "                             'was_fouled', 'fouls_committed',\n",
    "                             'distance_covered', 'defensive_contributions']].copy()\n",
    "\n",
    "# forward stats\n",
    "fwd_stats = pms_players_fwd[['player_id', 'match_id', 'team_code', 'Game Week', 'minutes_played',\n",
    "                             'goals', 'assists', 'xg', 'xa', 'xgot',\n",
    "                             'accurate_passes', 'final_third_passes',\n",
    "                             'total_shots', 'shots_on_target',\n",
    "                             'chances_created', 'big_chances_missed', 'touches', 'touches_opposition_box',\n",
    "                             'successful_dribbles', 'corners', 'offsides',\n",
    "                             'penalties_scored', 'penalties_missed',\n",
    "                             'duels_won', 'ground_duels_won', 'aerial_duels_won',\n",
    "                             'was_fouled', 'fouls_committed', 'dispossessed']].copy()\n",
    "\n",
    "print(f\"Shape of GK stats: {gk_stats.shape}\")\n",
    "print(f\"Shape of DEF stats: {def_stats.shape}\")\n",
    "print(f\"Shape of MID stats: {mid_stats.shape}\")\n",
    "print(f\"Shape of FWD stats: {fwd_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## Data Cleaning: Cleaning the data seperately for all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of GK stats: {gk_stats.shape}\")\n",
    "null_gk_stats_dict = {key: value for key, value in dict(gk_stats.isnull().sum()).items() if value > 0}\n",
    "print(f\"Columns with their NaN values: {null_gk_stats_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of DEF stats: {def_stats.shape}\")\n",
    "null_def_stats_dict = {key: value for key, value in dict(def_stats.isnull().sum()).items() if value > 0}\n",
    "print(f\"Columns with their NaN values: {null_def_stats_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of MID stats: {mid_stats.shape}\")\n",
    "null_mid_stats_dict = {key: value for key, value in dict(mid_stats.isnull().sum()).items() if value > 0}\n",
    "print(f\"Columns with their NaN values: {null_mid_stats_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of MID stats: {fwd_stats.shape}\")\n",
    "null_fwd_stats_dict = {key: value for key, value in dict(fwd_stats.isnull().sum()).items() if value > 0}\n",
    "print(f\"Columns with their NaN values: {null_fwd_stats_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to be dropped entirely from the dataset\n",
    "def_mid_drop_columns = ['defensive_contributions', 'distance_covered']\n",
    "fwd_drop_columns = ['dispossessed']\n",
    "\n",
    "def_stats = def_stats.drop(columns=def_mid_drop_columns, errors='ignore')\n",
    "mid_stats = mid_stats.drop(columns=def_mid_drop_columns, errors='ignore')\n",
    "fwd_stats = fwd_stats.drop(columns=fwd_drop_columns, errors='ignore')\n",
    "\n",
    "# features to be filled and imputed with 0\n",
    "gk_stats['saves_inside_box'] = gk_stats['saves_inside_box'].fillna(0)\n",
    "def_stats['tackles_won_percentage'] = def_stats['tackles_won_percentage'].fillna(0)\n",
    "mid_stats['corners'] = mid_stats['corners'].fillna(0) # check before & after imputing\n",
    "fwd_stats['corners'] = fwd_stats['corners'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of GK stats: {gk_stats.shape}\")\n",
    "print(f\"Shape of DEF stats: {def_stats.shape}\")\n",
    "print(f\"Shape of MID stats: {mid_stats.shape}\")\n",
    "print(f\"Shape of FWD stats: {fwd_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## Feature Engineering: Introducing Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the player was a genuine and active participant in the match, having played atleast 60 mins out of 90\n",
    "min_mins_played = 60\n",
    "genuine_gk_stats = gk_stats[gk_stats['minutes_played'] >= min_mins_played].copy()\n",
    "genuine_def_stats = def_stats[def_stats['minutes_played'] >= min_mins_played].copy()\n",
    "genuine_mid_stats = mid_stats[mid_stats['minutes_played'] >= min_mins_played].copy()\n",
    "genuine_fwd_stats = fwd_stats[fwd_stats['minutes_played'] >= min_mins_played].copy()\n",
    "print(f\"Shape of GK stats: {genuine_gk_stats.shape}\")\n",
    "print(f\"Shape of DEF stats: {genuine_def_stats.shape}\")\n",
    "print(f\"Shape of MID stats: {genuine_mid_stats.shape}\")\n",
    "print(f\"Shape of FWD stats: {genuine_fwd_stats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_gk_stats = genuine_gk_stats.sort_values(by=['player_id', 'Game Week'])\n",
    "genuine_gk_stats = genuine_gk_stats.reset_index(drop=True)\n",
    "genuine_gk_stats['chron_idx'] = genuine_gk_stats.index\n",
    "\n",
    "genuine_def_stats = genuine_def_stats.sort_values(by=['player_id', 'Game Week'])\n",
    "genuine_def_stats = genuine_def_stats.reset_index(drop=True)\n",
    "genuine_def_stats['chron_idx'] = genuine_def_stats.index\n",
    "\n",
    "genuine_mid_stats = genuine_mid_stats.sort_values(by=['player_id', 'Game Week'])\n",
    "genuine_mid_stats = genuine_mid_stats.reset_index(drop=True)\n",
    "genuine_mid_stats['chron_idx'] = genuine_mid_stats.index\n",
    "\n",
    "genuine_fwd_stats = genuine_fwd_stats.sort_values(by=['player_id', 'Game Week'])\n",
    "genuine_fwd_stats = genuine_fwd_stats.reset_index(drop=True)\n",
    "genuine_fwd_stats['chron_idx'] = genuine_fwd_stats.index\n",
    "\n",
    "print(\"All datasets are now sorted and contain the 'chron_idx' merge key without using inplace=True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the list of columns which will be engineered and transformed into rolling features (last 5)\n",
    "gk_rolling = ['gk_accurate_passes', 'gk_accurate_long_balls', \n",
    "              'saves', 'saves_inside_box', \n",
    "              'goals_conceded', 'team_goals_conceded', \n",
    "              'xgot_faced', 'goals_prevented' ,\n",
    "              'sweeper_actions', 'high_claim']\n",
    "def_rolling = ['xg', 'xa', 'accurate_passes', 'accurate_long_balls', 'final_third_passes',\n",
    "               'tackles_won', 'interceptions', 'recoveries', 'blocks', 'clearances',\n",
    "               'headed_clearances', 'dribbled_past', 'duels_won', 'ground_duels_won',\n",
    "               'aerial_duels_won', 'was_fouled', 'fouls_committed',\n",
    "               'tackles_won_percentage']\n",
    "mid_rolling = ['goals', 'assists', 'xg', 'xa', \n",
    "               'accurate_passes', 'accurate_crosses', 'accurate_long_balls','final_third_passes', \n",
    "               'total_shots', 'shots_on_target',\n",
    "               'chances_created', 'touches', 'successful_dribbles', 'corners',\n",
    "               'penalties_scored', 'penalties_missed', 'tackles_won', 'interceptions',\n",
    "               'recoveries', 'blocks', 'clearances', 'dribbled_past', 'duels_won',\n",
    "               'ground_duels_won', 'aerial_duels_won', 'was_fouled', 'fouls_committed']\n",
    "fwd_rolling = ['goals', 'assists', 'xg', 'xa', 'xgot', \n",
    "               'accurate_passes', 'final_third_passes', \n",
    "               'total_shots', 'shots_on_target', 'chances_created', 'big_chances_missed', \n",
    "               'touches', 'touches_opposition_box', 'successful_dribbles', 'corners', 'offsides',\n",
    "               'penalties_scored', 'penalties_missed', 'duels_won', 'ground_duels_won',\n",
    "               'aerial_duels_won', 'was_fouled', 'fouls_committed']\n",
    "\n",
    "def rollingFeatures(frame, rolling, groupedby='player_id', prefix='L5_Avg_'):\n",
    "    rolling_frame = frame.groupby(groupedby)[rolling].rolling(window=5, min_periods=1).mean().shift(1).reset_index()\n",
    "    new_cols = ['player_id', 'chron_idx'] + [prefix + col for col in rolling]\n",
    "    rolling_frame.columns = new_cols\n",
    "    first_row_idx = frame.groupby(groupedby)['chron_idx'].min().values\n",
    "    rolling_frame['is_first_match'] = rolling_frame['chron_idx'].isin(first_row_idx)\n",
    "    for col in rolling_frame.columns:\n",
    "        if col.startswith(prefix):\n",
    "            rolling_frame.loc[rolling_frame['is_first_match'], col] = np.nan\n",
    "    return frame.merge(\n",
    "        rolling_frame.drop(columns='is_first_match', errors='ignore'),\n",
    "        on=['player_id', 'chron_idx'], \n",
    "        how='left'\n",
    "    ).drop(columns='chron_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_gk_stats = rollingFeatures(genuine_gk_stats, gk_rolling)\n",
    "fe_def_stats = rollingFeatures(genuine_def_stats, def_rolling)\n",
    "fe_mid_stats = rollingFeatures(genuine_mid_stats, mid_rolling)\n",
    "fe_fwd_stats = rollingFeatures(genuine_fwd_stats, fwd_rolling)\n",
    "print(f\"Shape of Feature Engineered GK stats: {fe_gk_stats.shape}\")\n",
    "print(f\"Shape of Feature Engineered DEF stats: {fe_def_stats.shape}\")\n",
    "print(f\"Shape of Feature Engineered MID stats: {fe_mid_stats.shape}\")\n",
    "print(f\"Shape of Feature Engineered FWD stats: {fe_fwd_stats.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Merging Teams and Matches data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_strength_columns = ['name', 'code', 'season', 'strength', 'strength_overall_home', 'strength_attack_home', 'strength_defence_home', 'elo']\n",
    "away_strength_columns = ['name', 'code', 'season', 'strength', 'strength_overall_away', 'strength_attack_away', 'strength_defence_away', 'elo']\n",
    "teams_matches = combined_matches.merge(\n",
    "    teams_stats[home_strength_columns].rename(\n",
    "        columns={col: f'HT_{col}' for col in home_strength_columns if col not in ['code', 'season']}\n",
    "    ),\n",
    "    left_on=['home_team', 'season'],\n",
    "    right_on=['code', 'season'],\n",
    "    how='left'\n",
    ").drop(columns='code', errors='ignore').copy()\n",
    "teams_matches = teams_matches.merge(\n",
    "    teams_stats[away_strength_columns].rename(\n",
    "        columns={col: f'AT_{col}' for col in away_strength_columns if col not in ['code', 'season']}\n",
    "    ),\n",
    "    left_on=['away_team', 'season'],\n",
    "    right_on=['code', 'season'],\n",
    "    how='left'\n",
    ").drop(columns='code', errors='ignore').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Feature Reduction and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches['kickoff_time'] = pd.to_datetime(teams_matches['kickoff_time'], format=\"mixed\")\n",
    "teams_matches['gameweek'] = teams_matches['gameweek'].astype(int)\n",
    "teams_matches = teams_matches.sort_values(by=['gameweek', 'kickoff_time'], ascending=True).reset_index(drop=True)\n",
    "teams_matches = teams_matches.drop(columns='fotmob_id', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "#### Identifying Null Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "{key: value for key, value in dict(teams_matches.isnull().sum()).items() if value > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_elo_median = teams_matches['home_team_elo'].median()\n",
    "at_elo_median = teams_matches['away_team_elo'].median()\n",
    "teams_matches['home_team_elo'] = teams_matches['home_team_elo'].fillna(ht_elo_median)\n",
    "teams_matches['away_team_elo'] = teams_matches['away_team_elo'].fillna(at_elo_median)\n",
    "teams_matches['elo_diff'] = teams_matches['home_team_elo'] - teams_matches['away_team_elo']\n",
    "teams_matches = teams_matches.rename(columns={'home_team_elo':'ht_match_elo', 'away_team_elo':'at_match_elo'})\n",
    "print(f\"ELO ratings of Home Team imputed with {ht_elo_median:.2f} and Away Team imputed with {at_elo_median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cols = [\n",
    "    'home_possession', 'away_possession', \n",
    "    'home_tackles_won_pct', 'away_tackles_won_pct'\n",
    "]\n",
    "for col in median_cols:\n",
    "    median = teams_matches[col].median() \n",
    "    print(f\"Filled {teams_matches[col].isnull().sum()} NaNs in '{col}' with median: {median:.2f}\")\n",
    "    teams_matches[col] = teams_matches[col].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "{key: value for key, value in dict(teams_matches.isnull().sum()).items() if value > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(teams_matches.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms_players.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## Save all datasets as pickle file for Data Integration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "1. Position-wise Players Match Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_gk_stats.to_pickle('C:/exp1/Pickle Files/fe_gk_stats.pkl')\n",
    "fe_def_stats.to_pickle('C:/exp1/Pickle Files/fe_def_stats.pkl')\n",
    "fe_mid_stats.to_pickle('C:/exp1/Pickle Files/fe_mid_stats.pkl')\n",
    "fe_fwd_stats.to_pickle('C:/exp1/Pickle Files/fe_fwd_stats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "2. Teams + Match Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches.to_pickle('C:/exp1/Pickle Files/teams_matches.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final venv311",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
