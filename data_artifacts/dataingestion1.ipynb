{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eafee71-ee50-4f1c-9299-8e435df1c1c1",
   "metadata": {},
   "source": [
    "## I. Data Ingestion\n",
    "#### This marks the first step towards the development of our project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d93ee9-bfb0-49c8-a5c2-df9f694b8297",
   "metadata": {},
   "source": [
    "#### 1. Import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ae60ac-10cf-41fd-8a88-e8965e226248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # file and path handling\n",
    "import glob # get list of all CSV files present in the folder\n",
    "import numpy as np # for mathematical computations and vectorized summarizations\n",
    "import pandas as pd # data cleaning\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # for encoding categorical columns\n",
    "from sklearn.utils.class_weight import compute_class_weight # balanced weighting\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV # training and testing split, fine-tuning techniques\n",
    "from sklearn.metrics import accuracy_score, classification_report # performance of the model\n",
    "from sklearn.ensemble import RandomForestRegressor # for regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error # for error checking\n",
    "import xgboost as xgb # for classification\n",
    "import requests # using the github rest api for data retrieval\n",
    "import pickle # for importing pickle files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68867f8-5f09-4031-b7c2-a4a93c0a9ca2",
   "metadata": {},
   "source": [
    "#### 2. Extract Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5579310c-2418-4da8-93ec-1a8bb0eb288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'C:\\PROJECT\\data\\raw-data\\match-data-1' \n",
    "files = glob.glob(os.path.join(directory, 'pl*.csv')) # extract all the csv files present inside the folder \n",
    "files = sorted(files, key=lambda x: int(os.path.basename(x).replace('pl', '').replace('.csv', ''))) # sorting the files by 'pl' name\n",
    "good_df = [] # list to maintain all the error-free dataframes\n",
    "bad_files = [] # list to maintain all the bad files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cc61eb-5fb2-4784-96a1-e2c534e6262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect errors while reading the csv files one by one\n",
    "def checkError(files):\n",
    "    for file in files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file)\n",
    "            good_df.append(temp_df)\n",
    "            print(f\"Successfully read file: {file}\")\n",
    "        except Exception as e:\n",
    "            bad_files.append(file)\n",
    "            print(f\"Error occured: {e}, in file: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d4ab0d-c800-4bf3-861c-5289fbe4e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl0.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl1.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl2.csv\n",
      "Error occured: Error tokenizing data. C error: Expected 57 fields in line 305, saw 72\n",
      ", in file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl3.csv\n",
      "Error occured: 'utf-8' codec can't decode byte 0xa0 in position 75614: invalid start byte, in file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl4.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl5.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl6.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl7.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl8.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl9.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl10.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl11.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl12.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl13.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl14.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl15.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl16.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl17.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl18.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl19.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl20.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl21.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl22.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl23.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl24.csv\n",
      "Successfully read file: C:\\PROJECT\\data\\raw-data\\match-data-1\\pl25.csv\n"
     ]
    }
   ],
   "source": [
    "checkError(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4135d775-fe0a-4055-8216-df6dba89dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing errors in 2003-04 file\n",
    "file1 = r\"..\\Datasets\\pl3.csv\"\n",
    "pl3 = pd.read_csv(file1, sep=\",\", engine='python', header=0, on_bad_lines=\"skip\")\n",
    "good_df.insert(3, pl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b8e048-4970-4329-8440-0cffc08d452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing errors in 2004-05 file \n",
    "file2 = r\"..\\Datasets\\pl4.csv\"\n",
    "pl4 = pd.read_csv(file2, sep=\",\", engine='python', header=0, on_bad_lines=\"skip\", encoding=\"latin1\")\n",
    "good_df.insert(4, pl4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357be74e-9ff1-4e33-aad2-de6de6800fda",
   "metadata": {},
   "source": [
    "#### 3. Merging the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adecc731-8758-40bd-af5f-d522a6379a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 PL dataset's shape = (380, 45)\n",
      "2001 PL dataset's shape = (380, 48)\n",
      "2002 PL dataset's shape = (380, 53)\n",
      "2003 PL dataset's shape = (335, 57)\n",
      "2004 PL dataset's shape = (335, 57)\n",
      "2005 PL dataset's shape = (380, 68)\n",
      "2006 PL dataset's shape = (380, 68)\n",
      "2007 PL dataset's shape = (380, 71)\n",
      "2008 PL dataset's shape = (380, 71)\n",
      "2009 PL dataset's shape = (380, 71)\n",
      "2010 PL dataset's shape = (380, 71)\n",
      "2011 PL dataset's shape = (380, 71)\n",
      "2012 PL dataset's shape = (380, 74)\n",
      "2013 PL dataset's shape = (380, 68)\n",
      "2014 PL dataset's shape = (381, 68)\n",
      "2015 PL dataset's shape = (380, 65)\n",
      "2016 PL dataset's shape = (380, 65)\n",
      "2017 PL dataset's shape = (380, 65)\n",
      "2018 PL dataset's shape = (380, 62)\n",
      "2019 PL dataset's shape = (380, 106)\n",
      "2020 PL dataset's shape = (380, 106)\n",
      "2021 PL dataset's shape = (380, 106)\n",
      "2022 PL dataset's shape = (380, 106)\n",
      "2023 PL dataset's shape = (380, 106)\n",
      "2024 PL dataset's shape = (380, 120)\n",
      "2025 PL dataset's shape = (60, 132)\n"
     ]
    }
   ],
   "source": [
    "# analyzing the difference in shapes of all datasets\n",
    "df_list = good_df.copy()\n",
    "start = 2000\n",
    "for temp_df in df_list:\n",
    "    print(f\"{start} PL dataset's shape = {temp_df.shape}\")\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e72e2e0-e55f-4485-95b8-78a33a2c2e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape after merging raw datasets = (9471, 219)\n"
     ]
    }
   ],
   "source": [
    "original_df = pd.concat(good_df, ignore_index=True)\n",
    "print(f\"Original shape after merging raw datasets = {original_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76874b82-ffea-4c29-93c1-5b8d6c009f0c",
   "metadata": {},
   "source": [
    "## II. Data Structuring\n",
    "#### This step comprises of techniques to refine the structure of the dataset and reduce complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c791b687-0e37-40c7-9f37-46eddf67df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a list of columns that need to be removed to reduce redundancy and noise from the data set\n",
    "removable = ['Div', 'AHCh', 'HHW', 'AHW', 'HO', 'AO',\n",
    "             'IWH', 'IWD', 'IWA', \n",
    "             'LBH', 'LBD', 'LBA', \n",
    "             'SBH', 'SBD', 'SBA', \n",
    "             'WHH', 'WHD', 'WHA', \n",
    "             'SYH', 'SYD', 'SYA', \n",
    "             'SOH', 'SOD', 'SOA',\n",
    "             'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', \n",
    "             'GBAHH', 'GBAHA', 'GBAH', \n",
    "             'LBAHH', 'LBAHA', 'LBAH', \n",
    "             'Bb1X2', 'BbOU', 'BbAH', 'BbMxAHH', 'BbMxAHA',\n",
    "             'BWH', 'BWD', 'BWA', \n",
    "             'SJH', 'SJD', 'SJA', \n",
    "             'VCH', 'VCD', 'VCA',\n",
    "             'BSH', 'BSD', 'BSA', \n",
    "             'Time',\n",
    "             'BWCH', 'BWCD', 'BWCA', \n",
    "             'IWCH', 'IWCD', 'IWCA', \n",
    "             'WHCH', 'WHCD', 'WHCA', \n",
    "             'VCCH', 'VCCD', 'VCCA',\n",
    "             '1XBH', '1XBD', '1XBA', \n",
    "             '1XBCH', '1XBCD', '1XBCA', \n",
    "             'BFH', 'BFD', 'BFA', \n",
    "             'BFEH', 'BFED', 'BFEA', \n",
    "             'BFE>2.5', 'BFE<2.5', \n",
    "             'BFEAHH', 'BFEAHA',\n",
    "             'BFDH', 'BFDD', 'BFDA',\n",
    "             'BMGMH','BMGMD','BMGMA',\n",
    "             'BVH','BVD','BVA',\n",
    "             'CLH','CLD','CLA',\n",
    "             'BFDCH','BFDCD','BFDCA',\n",
    "             'BMGMCH','BMGMCD','BMGMCA',\n",
    "             'BVCH','BVCD','BVCA',\n",
    "             'CLCH','CLCD','CLCA',\n",
    "             'LBCH', 'LBCD','LBCA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19dfe7c7-10e0-4233-a3ac-c08baa44c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised shape after filtering of columns = (9471, 111)\n"
     ]
    }
   ],
   "source": [
    "revised_df = original_df.drop(columns=removable, errors='ignore')\n",
    "print(f\"Revised shape after filtering of columns = {revised_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff3289-ddc7-4c58-a5c9-783f9671f710",
   "metadata": {},
   "source": [
    "## III. Feature Engineering (I)\n",
    "#### In this step, certain features will undergo engineered transformations to add more quality to the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34559d-664a-410e-8562-16abbc35a0bd",
   "metadata": {},
   "source": [
    "#### Features involving Betting Odds must be converted to Implied Probability and then further normalized, requiring intensive feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fb45a6-87e9-4cb0-a3e6-5beb3dd18127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Implied Probabilities (IP) using PSCH/D/A\n",
    "# We need these temporary IP columns to compute the margin\n",
    "revised_df = revised_df.copy()\n",
    "\n",
    "revised_df['IP_Home_PS'] = 1 / revised_df['PSCH']\n",
    "revised_df['IP_Draw_PS'] = 1 / revised_df['PSCD']\n",
    "revised_df['IP_Away_PS'] = 1 / revised_df['PSCA']\n",
    "\n",
    "# 2. Calculate Market Margin\n",
    "# Margin is the sum of Implied Probabilities minus 1.\n",
    "revised_df['NormIP_Margin'] = (revised_df['IP_Home_PS'] + revised_df['IP_Draw_PS'] + revised_df['IP_Away_PS']) - 1\n",
    "\n",
    "# 3. Cleanup: Drop the temporary IP columns\n",
    "# The NormIP_Margin feature is now permanently added to the DataFrame.\n",
    "revised_df = revised_df.drop(columns=['IP_Home_PS', 'IP_Draw_PS', 'IP_Away_PS'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f1554-defb-46b4-8023-8848337c712b",
   "metadata": {},
   "source": [
    "### Check the final list of odds and only engineer those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c24012-4f41-4234-b3c2-d51bd5e4a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a list of columns that needs to be engineered and transformed\n",
    "cols = list(revised_df.columns)\n",
    "\n",
    "# Odds -> IP -> Normalization\n",
    "\n",
    "# 1. Classification Odds\n",
    "open_market = ['BbAvH', 'BbAvD', 'BbAvA'] # opening market average odds (consensus price when the market opens\n",
    "close_market_classify = ['AvgCH', 'AvgCD', 'AvgCA'] # closing market average odds (consensus price at kick-off) (critical for classification)\n",
    "pinnacle_classification = ['PSCH', 'PSCD', 'PSCA'] # sharp closing odds by pinnacle\n",
    "newly_added_odds = ['MaxH', 'MaxD', 'MaxA', 'AvgH', 'AvgD', 'AvgA', 'B365H', 'B365D', 'B365A', 'B365CH', 'B365CD', 'B365CA']\n",
    "\n",
    "# 2. Regression Odds\n",
    "close_market_goals = ['AvgC>2.5', 'AvgC<2.5'] # closing market average odds (critical for goal prediction) (over or under 2.5 goals)\n",
    "pinnacle_regression = ['PC>2.5', 'PC<2.5'] # sharp closing odds by pinnacle\n",
    "\n",
    "# asian handicap odds specialize in the elimination of the possibility of a match being drawn. they do not require normalization\n",
    "# it does by applying a handicap (goal deficit or advantage) to one or more teams, right before kick-off\n",
    "closing_average = ['AvgCAHH', 'AvgCAHA'] # closing market average odds\n",
    "pinnacle_asian = ['PCAHH', 'PCAHA'] # asian handicap odds by pinnacle\n",
    "\n",
    "prob_norm_odds = [open_market, close_market_classify, pinnacle_classification,\n",
    "                  newly_added_odds, close_market_goals, pinnacle_regression]\n",
    "\n",
    "prob_only_odds = [closing_average, pinnacle_asian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9ce30e-fd7b-4048-b7a1-fab65d9becc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for remaining odds\n",
    "def Probability_Normalization(temp_df, columns):\n",
    "    ip_columns = [f'IP_{col}' for col in columns]\n",
    "    for odd, new in zip(columns, ip_columns):\n",
    "        temp_df[new] = 1 / temp_df[odd]\n",
    "    total = temp_df[ip_columns].sum(axis=1)\n",
    "    norm_columns = [f'NormIP_{col}' for col in columns]\n",
    "    for ip, norm in zip(ip_columns, norm_columns):\n",
    "        temp_df[norm] = temp_df[ip] / total\n",
    "    temp_df = temp_df.drop(columns=columns + ip_columns)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31ead07-3ce2-49dd-90da-c98cb64cb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for asian handicap odds \n",
    "def Probability(temp_df, columns):\n",
    "    ip_columns = [f'IP_AHO_{col}' for col in columns]\n",
    "    for odd, new in zip(columns, ip_columns):\n",
    "        temp_df[new] = 1 / temp_df[odd]\n",
    "    temp_df = temp_df.drop(columns=columns)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16827a49-6f79-4aa1-9f90-f29848ae2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (9471, 112)\n"
     ]
    }
   ],
   "source": [
    "df = revised_df.copy()\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da3259-3fe1-4d14-bd84-54e5c186b253",
   "metadata": {},
   "source": [
    "### Forming new columns, based on Lag & Rolling features and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8c408-a51f-439c-8337-6589e4f4991f",
   "metadata": {},
   "source": [
    "#### 1. TEAM SPECIFIC - GOALS FOR & GOALS AGAINST\n",
    "##### 'Team_GF_L10' = Average Goals scored by the team in the last 10 matches\n",
    "##### 'Team_GA_L10' = Average Goals conceded by the team in the last 10 matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd3cae-4f6f-4cde-b9f1-d557a61184f4",
   "metadata": {},
   "source": [
    "#### 2. HOME - GOALS FOR & GOALS AGAINST\n",
    "##### 'HT_AvgGF_L5' = Average Goals scored by the team in its Home ground the last 5 matches\n",
    "##### 'HT_AvgGA_L5' = Average Goals conceded by the team in its Home ground the last 5 matches\n",
    "##### 'HT_AvgGD_L5' = Average Goal difference by the team in its Home ground the last 5 matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05100a7-8116-4a0a-9148-c9ec72ce0056",
   "metadata": {},
   "source": [
    "#### 3. AWAY- GOALS FOR & GOALS AGAINST\n",
    "##### 'AT_AvgGF_L5' = Average Goals scored by the team in Away ground in the last 5 matches \n",
    "##### 'AT_AvgGA_L5' = Average Goals conceded by the team in Away ground in the last 5 matches\n",
    "##### 'AT_AvgGD_L5' = Average Goal difference by the team in Away ground in the last 5 matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5c7b3-0e9d-4734-a583-3b84198dc2bd",
   "metadata": {},
   "source": [
    "#### 4. HOME TEAM & AWAY TEAM - CLEAN SHEET RECORD (LAST 5 MATCHES)\n",
    "##### 'HT_CS_L5' = Proportion of Home Team Clean Sheets in the last 5 matches (no. of clean sheets in L5 / 5)\n",
    "##### 'AT_CS_L5' = Proportion of Away Team Clean Sheets in the last 5 matches (no. of clean sheets in L5 / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecef8e-5e6d-4263-b82f-a41a0a7a9b17",
   "metadata": {},
   "source": [
    "#### 5. HOME TEAM & AWAY TEAM - AVERAGE SHOTS (LAST 5 MATCHES)\n",
    "##### 'HT_AvgShots_L5' = Average Shots taken by the Home Team in the last 5 matches\n",
    "##### 'AT_AvgShots_L5' = Average Shots taken by the Away Team in the Last 5 matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b88cd-9551-48af-bb1c-dac7712e7941",
   "metadata": {},
   "source": [
    "#### 6. HOME TEAM & AWAY TEAM - SHOT ACCURACY (LAST 5 MATCHES)\n",
    "##### 'HT_ShotAccuracy_L5' = 'HST' / 'HS' (shots on target / total shots taken) (home team)\n",
    "##### 'AT_ShotAccuracy_L5' = 'AST' / 'AS' (shots on target / total shots taken) (away team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38a070-9e4a-48f9-a7d4-a48f84dac985",
   "metadata": {},
   "source": [
    "#### 7. HOME TEAM & AWAY TEAM - SHOT CONVERSION (LAST 5 MATCHES) \n",
    "##### 'HT_ShotConversion_L5' = Goals scored / Total shots taken, by the Home Team (last 5 matches)\n",
    "##### 'AT_ShotConversion_L5' = Goals scored / Total shots taken, by the Away Team (last 5 matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e5408-d366-4458-a8e5-a37e1830b9c8",
   "metadata": {},
   "source": [
    "#### 8. HOME TEAM & AWAY TEAM - WIN PROPORTION (LAST 5 MATCHES)\n",
    "##### 'HT_WinRate_L5' = Win Proportion of Home Team in the last 5 matches (no. of wins in L5 / 5)\n",
    "##### 'AT_WinRate_L5' = Win Proportion of Away Team in the last 5 matches (no. of wins in L5 / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109b015-51ad-44f6-a3d9-630f8de3e762",
   "metadata": {},
   "source": [
    "#### 9. TOTAL AVERAGE CARDS GIVEN BY THE REFEREE\n",
    "##### 'Ref_Avg_Cards' = Average number of Yellow & Red cards given by the referee prior to the match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2226a-eb71-4f76-b7ca-f81d9c48e68a",
   "metadata": {},
   "source": [
    "#### 10. INDICATOR FOR THE LAST WIN OF HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09cedc-4156-4a57-9f00-ae2cd43d3ed0",
   "metadata": {},
   "source": [
    "#### Storing the dataset in ascending order for Lag & Rolling features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7e0250-8d47-4d9c-9a72-b195c59d205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DateConversion(col):\n",
    "    date = str(col).strip()\n",
    "    if len(date.split(\"/\")[-1]) == 2:\n",
    "        return pd.to_datetime(col, format=\"%d/%m/%y\", dayfirst=True)\n",
    "    else: return pd.to_datetime(col, format=\"%d/%m/%Y\", dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3d98fb-db1d-4ef5-a4d1-ccd11ad1f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].apply(DateConversion)\n",
    "df = df.sort_values('Date', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b911a-02d8-4b6f-8be7-3c33f9dff5a1",
   "metadata": {},
   "source": [
    "#### 1. TEAM SPECIFIC - GF & GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43e1e039-bfc7-45c1-997b-39f697bd3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe consisting of home team statistics\n",
    "home_df = df[['Date', 'HomeTeam', 'FTHG', 'FTAG']].copy()\n",
    "home_df.columns = ['Date', 'Team', 'GoalsFor', 'GoalsAgainst'] # renaming the home_df columns\n",
    "home_df['Venue'] = 'Home'\n",
    "\n",
    "# dataframe consisting of away team statistics\n",
    "away_df = df[['Date', 'AwayTeam', 'FTAG', 'FTHG']].copy()\n",
    "away_df.columns = ['Date', 'Team', 'GoalsFor', 'GoalsAgainst'] # renaming the away_df columns\n",
    "away_df['Venue'] = 'Away'\n",
    "\n",
    "# combining both the dataframes\n",
    "combined_df = pd.concat([home_df, away_df], ignore_index=True)\n",
    "combined_df = combined_df.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# compute the total goals scored by the team in the past 5 matches \n",
    "combined_df['Team_GF_L5'] = combined_df.groupby('Team')['GoalsFor'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "\n",
    "# compute the total goals conceded by the team in the past 5 matches\n",
    "combined_df['Team_GA_L5'] = combined_df.groupby('Team')['GoalsAgainst'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "\n",
    "# home teams\n",
    "df = df.merge(combined_df[['Date', 'Team', 'Team_GF_L5', 'Team_GA_L5']], \n",
    "             left_on=['Date', 'HomeTeam'], \n",
    "             right_on=['Date', 'Team'],  \n",
    "             how='left') \n",
    "df = df.rename(columns={'Team_GF_L5': 'HT_AvgGF_L5', 'Team_GA_L5': 'HT_AvgGA_L5'})\n",
    "df = df.drop(columns='Team') \n",
    "\n",
    "# away teams\n",
    "df = df.merge(combined_df[['Date', 'Team', 'Team_GF_L5', 'Team_GA_L5']],\n",
    "             left_on=['Date', 'AwayTeam'],\n",
    "             right_on=['Date', 'Team'],\n",
    "             how='left') # keep all rows of main df, even if combined_df consists of some NaN values\n",
    "df = df.rename(columns={'Team_GF_L5': 'AT_AvgGF_L5', 'Team_GA_L5': 'AT_AvgGA_L5'})\n",
    "df = df.drop(columns='Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d16ee36-fd68-4be8-8c8b-c8ab30d22d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 1st stage of feature engineering) = (9474, 116)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 1st stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f60ea5-8fe6-4758-99f4-e551171d87ec",
   "metadata": {},
   "source": [
    "#### 2. HOME GROUND SPECIFICS - GOALS FOR & GOALS AGAINST & GOAL DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fcdcc7-afa2-4e03-bf03-4e74e38ad046",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_ground = df[['Date', 'HomeTeam', 'FTHG', 'FTAG']].copy()\n",
    "home_ground = home_ground.rename(columns={'HomeTeam':'Team', 'FTHG':'GoalsFor', 'FTAG':'GoalsAgainst'})\n",
    "home_ground = home_ground.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "home_ground['HG_HT_AvgGF_L5'] = home_ground.groupby('Team')['GoalsFor'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "home_ground['HG_HT_AvgGA_L5'] = home_ground.groupby('Team')['GoalsAgainst'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "home_ground['HG_HT_AvgGD_L5'] = home_ground['HG_HT_AvgGF_L5'] - home_ground['HG_HT_AvgGA_L5']\n",
    "df = df.merge(home_ground[['Date', 'Team', 'HG_HT_AvgGF_L5', 'HG_HT_AvgGA_L5', 'HG_HT_AvgGD_L5']],\n",
    "             left_on=['Date', 'HomeTeam'],\n",
    "             right_on=['Date', 'Team'],\n",
    "             how='left')\n",
    "df = df.drop(columns='Team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfb95c-2cbe-44c3-8a19-3b6b4ce55b4f",
   "metadata": {},
   "source": [
    "#### 3. AWAY GROUND SPECIFICS - GOALS FOR & GOALS AGAINST & GOAL DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fce0b30-6fb7-420d-8912-23d9600a0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "away_ground = df[['Date', 'AwayTeam', 'FTHG', 'FTAG']].copy()\n",
    "away_ground = away_ground.rename(columns={'AwayTeam':'Team', 'FTHG':'GoalsAgainst', 'FTAG':'GoalsFor'})\n",
    "away_ground = away_ground.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "away_ground['AG_AT_AvgGF_L5'] = away_ground.groupby('Team')['GoalsFor'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "away_ground['AG_AT_AvgGA_L5'] = away_ground.groupby('Team')['GoalsAgainst'].transform(lambda x: x.shift().rolling(window=5, min_periods=1).mean())\n",
    "away_ground['AG_AT_AvgGD_L5'] = away_ground['AG_AT_AvgGF_L5'] - away_ground['AG_AT_AvgGA_L5']\n",
    "df = df.merge(away_ground[['Date', 'Team', 'AG_AT_AvgGF_L5', 'AG_AT_AvgGA_L5', 'AG_AT_AvgGD_L5']],\n",
    "             left_on=['Date', 'AwayTeam'],\n",
    "             right_on=['Date', 'Team'],\n",
    "             how='left')\n",
    "df = df.drop(columns='Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abbbcbd7-00b7-409b-adc0-66570bbc1cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 2nd & 3rd stages of feature engineering) = (9726, 122)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 2nd & 3rd stages of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7fdc968-98a5-42ef-bf62-6dedad555157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c3b2ce-2867-4125-a59b-f33d3d2a63c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after removing duplicates) = (9471, 122)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after removing duplicates) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94493bfd-35a2-41d6-aa9c-f8ee9eeacd71",
   "metadata": {},
   "source": [
    "#### 4. AVERAGE SHOTS FOR HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad5c563b-8bc1-4193-96fc-3c1173baa210",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_home = df[['Date','HomeTeam','HS']].copy()\n",
    "shots_home.columns = ['Date','Team','Shots']\n",
    "shots_home['Venue'] = 'Home'\n",
    "\n",
    "shots_away = df[['Date','AwayTeam','AS']].copy()\n",
    "shots_away.columns = ['Date','Team','Shots']\n",
    "shots_away['Venue'] = 'Away'\n",
    "\n",
    "shots_df = pd.concat([shots_home, shots_away], ignore_index=True)\n",
    "shots_df = shots_df.sort_values(['Team','Date']).reset_index(drop=True)\n",
    "shots_df['Shots_L5'] = (shots_df.groupby('Team')['Shots'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean()))\n",
    "\n",
    "shots_df = shots_df[['Date','Team','Shots_L5']]\n",
    "shots_df = shots_df.drop_duplicates(subset=['Date','Team'])\n",
    "\n",
    "mapping = dict(zip(zip(shots_df['Date'], shots_df['Team']), shots_df['Shots_L5']))\n",
    "\n",
    "df['HT_AvgShots_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['HomeTeam'])), axis=1)\n",
    "df['AT_AvgShots_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['AwayTeam'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d6f8107-00a5-4b4a-80cb-f0124008ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 4th stage of feature engineering) = (9471, 124)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 4th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76f51a-b69d-4651-83fb-5d3f6e7f62ed",
   "metadata": {},
   "source": [
    "#### 5. SHOT ACCURACY - HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7cb42b-a6fb-4a6a-83ad-0660decdc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_shots = df[['Date', 'HomeTeam', 'HS', 'HST']].copy()\n",
    "home_shots = home_shots.rename(columns={'HomeTeam':'Team'})\n",
    "home_shots['Shot_Accuracy'] = home_shots['HST'] / home_shots['HS']\n",
    "home_shots = home_shots[['Date', 'Team', 'Shot_Accuracy']]\n",
    "\n",
    "away_shots = df[['Date', 'AwayTeam', 'AS', 'AST']].copy()\n",
    "away_shots = away_shots.rename(columns={'AwayTeam':'Team'})\n",
    "away_shots['Shot_Accuracy'] = away_shots['AST'] / away_shots['AS']\n",
    "away_shots = away_shots[['Date', 'Team', 'Shot_Accuracy']]\n",
    "\n",
    "shots_acc_df = pd.concat([home_shots, away_shots], ignore_index=True)\n",
    "shots_acc_df = shots_acc_df.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "shots_acc_df['ShotAccuracy_L5'] = shots_acc_df.groupby('Team')['Shot_Accuracy'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
    "\n",
    "shots_acc_df = shots_acc_df[['Date','Team','ShotAccuracy_L5']].drop_duplicates()\n",
    "\n",
    "mapping = dict(zip(zip(shots_acc_df['Date'], shots_acc_df['Team']), shots_acc_df['ShotAccuracy_L5']))\n",
    "\n",
    "df['HT_ShotAccuracy_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['HomeTeam'])), axis=1)\n",
    "df['AT_ShotAccuracy_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['AwayTeam'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cd7ee37-c7ab-4b4d-b980-e0d0e91c5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 5th stage of feature engineering) = (9471, 126)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 5th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a65cf1-9059-4542-8037-26e24186927e",
   "metadata": {},
   "source": [
    "#### 6. SHOT CONVERSION - HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03c16cff-2387-4129-90be-47a3b149e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_shots = df[['Date', 'HomeTeam', 'FTHG', 'HS']].copy()\n",
    "ht_shots = ht_shots.rename(columns={'HomeTeam':'Team'})\n",
    "ht_shots['Shot_Conversion'] = np.where(ht_shots['HS'] > 0, ht_shots['FTHG'] / ht_shots['HS'], 0)\n",
    "ht_shots = ht_shots[['Date', 'Team', 'Shot_Conversion']]\n",
    "\n",
    "at_shots = df[['Date', 'AwayTeam', 'FTAG', 'AS']].copy()\n",
    "at_shots = at_shots.rename(columns={'AwayTeam':'Team'})\n",
    "at_shots['Shot_Conversion'] = np.where(at_shots['AS'] > 0, at_shots['FTAG'] / at_shots['AS'], 0)\n",
    "at_shots = at_shots[['Date', 'Team', 'Shot_Conversion']]\n",
    "\n",
    "shots_conv_df = pd.concat([ht_shots, at_shots], ignore_index=True)\n",
    "shots_conv_df = shots_conv_df.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "shots_conv_df['ShotConversion_L5'] = shots_conv_df.groupby('Team')['Shot_Conversion'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
    "\n",
    "shots_conv_df = shots_conv_df[['Date','Team','ShotConversion_L5']].drop_duplicates()\n",
    "\n",
    "mapping = dict(zip(zip(shots_conv_df['Date'], shots_conv_df['Team']), shots_conv_df['ShotConversion_L5']))\n",
    "\n",
    "df['HT_ShotConversion_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['HomeTeam'])), axis=1)\n",
    "df['AT_ShotConversion_L5'] = df.apply(lambda x: mapping.get((x['Date'], x['AwayTeam'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e229a308-f3cf-4624-8a46-112d6b9ab2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 6th stage of feature engineering) = (9471, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 6th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085c088-d153-49d1-a310-354c3dfc0d9e",
   "metadata": {},
   "source": [
    "#### 7. CLEAN SHEET RECORD - HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd979b9-a841-4fe8-8687-1b361fbbf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_record = df[['Date', 'HomeTeam', 'FTAG']].copy()\n",
    "home_record = home_record.rename(columns={'HomeTeam':'Team', 'FTAG':'GoalsAgainst'})\n",
    "home_record['Clean_Sheet'] = np.where(home_record['GoalsAgainst'] > 0, 0, 1)\n",
    "home_record = home_record[['Date', 'Team', 'Clean_Sheet']]\n",
    "\n",
    "away_record = df[['Date', 'AwayTeam', 'FTHG']].copy()\n",
    "away_record = away_record.rename(columns={'AwayTeam':'Team', 'FTHG':'GoalsAgainst'})\n",
    "away_record['Clean_Sheet'] = np.where(away_record['GoalsAgainst'] > 0, 0, 1)\n",
    "away_record = away_record[['Date', 'Team', 'Clean_Sheet']]\n",
    "\n",
    "cs_record = pd.concat([home_record, away_record], ignore_index=True)\n",
    "cs_record = cs_record.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "cs_record['CleanSheet_L5'] = cs_record.groupby('Team')['Clean_Sheet'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
    "cs_record = cs_record[['Date', 'Team', 'CleanSheet_L5']].drop_duplicates()\n",
    "\n",
    "cs_mapping = dict(zip(zip(cs_record['Date'], cs_record['Team']), cs_record['CleanSheet_L5']))\n",
    "\n",
    "df['HT_CS_L5'] = df.apply(lambda x: cs_mapping.get((x['Date'], x['HomeTeam'])), axis=1)\n",
    "df['AT_CS_L5'] = df.apply(lambda x: cs_mapping.get((x['Date'], x['AwayTeam'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a3b3a7-a185-4404-ac11-becf2fdb53cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 7th stage of feature engineering) = (9471, 130)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 7th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340faf77-e183-411a-88ca-1b0eb0e3556d",
   "metadata": {},
   "source": [
    "#### 8. WIN RATE - HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e839162a-5cfc-4f64-a758-a3ec01f5f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_wins = df[['Date', 'HomeTeam', 'FTR']].copy()\n",
    "home_wins = home_wins.rename(columns={'HomeTeam':'Team'})\n",
    "home_wins['Win'] = np.where(home_wins['FTR'] == 'H', 1, 0)\n",
    "home_wins = home_wins[['Date', 'Team', 'Win']]\n",
    "\n",
    "away_wins = df[['Date', 'AwayTeam', 'FTR']].copy()\n",
    "away_wins = away_wins.rename(columns={'AwayTeam':'Team'})\n",
    "away_wins['Win'] = np.where(away_wins['FTR'] == 'A', 1, 0)\n",
    "away_wins = away_wins[['Date', 'Team', 'Win']]\n",
    "\n",
    "wins_record = pd.concat([home_wins, away_wins], ignore_index=True)\n",
    "wins_record = wins_record.sort_values(['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "wins_record['Wins_L5'] = wins_record.groupby('Team')['Win'].transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
    "wins_record = wins_record[['Date', 'Team', 'Wins_L5']].drop_duplicates()\n",
    "\n",
    "win_mapping = dict(zip(zip(wins_record['Date'], wins_record['Team']), wins_record['Wins_L5']))\n",
    "\n",
    "df['HT_WinRate_L5'] = df.apply(lambda x: win_mapping.get((x['Date'], x['HomeTeam'])), axis=1)\n",
    "df['AT_WinRate_L5'] = df.apply(lambda x: win_mapping.get((x['Date'], x['AwayTeam'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "383f2479-6e6a-4253-93b8-0cbbbdb32803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 8th stage of feature engineering) = (9471, 132)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 8th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9711be-2816-44f0-9ebf-50c56b12a256",
   "metadata": {},
   "source": [
    "#### 9. TOTAL AVERAGE CARDS GIVEN BY THE REFEREE PRIOR TO THE MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03bcf2ab-62f0-473c-8f46-f094c78401f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCards'] = (df['HY'] + df['AY'] + df['HR'] + df['AR'])\n",
    "df['Ref_Avg_Cards'] = df.groupby('Referee')['TotalCards'].transform(lambda x: x.expanding().mean().shift(1))\n",
    "df['Ref_Avg_Cards'] = df['Ref_Avg_Cards'].fillna(df['TotalCards'].median())\n",
    "df = df.drop(columns='TotalCards', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35c89555-aeef-4822-b9bb-afb5d8e565f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 9th stage of feature engineering) = (9471, 133)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 9th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6fbb6-79d7-47f8-9dff-60e3578d0885",
   "metadata": {},
   "source": [
    "#### 10. INDICATOR FOR THE LAST WIN OF HOME TEAM & AWAY TEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf65da6-adcc-438a-991b-56d886fec0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HT_Win_Indicator'] = (df['FTR'] == 'H').astype(int)\n",
    "df['AT_Win_Indicator'] = (df['FTR'] == 'A').astype(int)\n",
    "\n",
    "ht_wins = df[['Date', 'HomeTeam', 'HT_Win_Indicator']].rename(columns={'HomeTeam':'Team', 'HT_Win_Indicator':'Win'})\n",
    "at_wins = df[['Date', 'AwayTeam', 'AT_Win_Indicator']].rename(columns={'AwayTeam':'Team', 'AT_Win_Indicator':'Win'})\n",
    "\n",
    "combined = pd.concat([ht_wins, at_wins], ignore_index=True)\n",
    "combined = combined.sort_values(by=['Team', 'Date']).reset_index(drop=True)\n",
    "\n",
    "combined['Last_Win'] = combined.groupby('Team')['Win'].shift(1)\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    combined[['Team', 'Date', 'Last_Win']],\n",
    "    left_on=['HomeTeam', 'Date'],\n",
    "    right_on=['Team', 'Date'],\n",
    "    how='left'\n",
    ").rename(columns={'Last_Win': 'HT_Last_Win'}).drop(columns=['Team'])\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    combined[['Team', 'Date', 'Last_Win']],\n",
    "    left_on=['AwayTeam', 'Date'],\n",
    "    right_on=['Team', 'Date'],\n",
    "    how='left'\n",
    ").rename(columns={'Last_Win': 'AT_Last_Win'}).drop(columns=['Team'])\n",
    "\n",
    "df['HT_Last_Win'] = df['HT_Last_Win'].fillna(0)\n",
    "df['AT_Last_Win'] = df['AT_Last_Win'].fillna(0)\n",
    "\n",
    "df = df.drop(columns=['HT_Win_Indicator', 'AT_Win_Indicator'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21fbc3b-8eef-4e15-8e46-f98fafbe1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 10th stage of feature engineering) = (9474, 135)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 10th stage of feature engineering) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47767424-b167-49f4-a5df-264a6d7fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb2cc9a5-ed10-4f6c-9a4f-416fce7246a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (after 10th stage of feature engineering and dropping duplicates) = (9471, 135)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape (after 10th stage of feature engineering and dropping duplicates) = {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9120a17-de03-408b-b4ec-01fc299414bc",
   "metadata": {},
   "source": [
    "## IV. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba104e5-1af0-485d-9047-023416faaf30",
   "metadata": {},
   "source": [
    "#### 1. Deleting row having every value as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bacf14e5-ec09-40e4-8867-7ba026df2178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>...</th>\n",
       "      <th>AT_ShotAccuracy_L5</th>\n",
       "      <th>HT_ShotConversion_L5</th>\n",
       "      <th>AT_ShotConversion_L5</th>\n",
       "      <th>HT_CS_L5</th>\n",
       "      <th>AT_CS_L5</th>\n",
       "      <th>HT_WinRate_L5</th>\n",
       "      <th>AT_WinRate_L5</th>\n",
       "      <th>Ref_Avg_Cards</th>\n",
       "      <th>HT_Last_Win</th>\n",
       "      <th>AT_Last_Win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>2025-09-27</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.150054</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252904</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9468</th>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336084</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.126813</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.419643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Everton</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455952</td>\n",
       "      <td>0.118586</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.170732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9470</th>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     HomeTeam  AwayTeam  FTHG  FTAG  FTR  HTHG  HTAG  HTR  \\\n",
       "9466 2025-09-27    Tottenham    Wolves   1.0   1.0    D   0.0   0.0    D   \n",
       "9467 2025-09-28    Newcastle   Arsenal   1.0   2.0    A   1.0   0.0    H   \n",
       "9468 2025-09-28  Aston Villa    Fulham   3.0   1.0    H   1.0   1.0    D   \n",
       "9469 2025-09-29      Everton  West Ham   1.0   1.0    D   1.0   0.0    H   \n",
       "9470        NaT          NaN       NaN   NaN   NaN  NaN   NaN   NaN  NaN   \n",
       "\n",
       "      Attendance  ... AT_ShotAccuracy_L5  HT_ShotConversion_L5  \\\n",
       "9466         NaN  ...           0.316667              0.150054   \n",
       "9467         NaN  ...           0.252904              0.052500   \n",
       "9468         NaN  ...           0.336084              0.016667   \n",
       "9469         NaN  ...           0.455952              0.118586   \n",
       "9470         NaN  ...                NaN                   NaN   \n",
       "\n",
       "      AT_ShotConversion_L5  HT_CS_L5  AT_CS_L5  HT_WinRate_L5  AT_WinRate_L5  \\\n",
       "9466              0.045833       0.6       0.0            0.6            0.0   \n",
       "9467              0.131944       0.8       0.6            0.2            0.6   \n",
       "9468              0.126813       0.4       0.2            0.0            0.4   \n",
       "9469              0.091667       0.4       0.2            0.4            0.2   \n",
       "9470                   NaN       NaN       NaN            NaN            NaN   \n",
       "\n",
       "      Ref_Avg_Cards  HT_Last_Win  AT_Last_Win  \n",
       "9466       3.812500          0.0          0.0  \n",
       "9467       3.769231          0.0          0.0  \n",
       "9468       3.419643          0.0          1.0  \n",
       "9469       4.170732          0.0          0.0  \n",
       "9470       3.000000          0.0          0.0  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb90fe8d-d6ef-44a4-8465-140c2c396600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce0b1e6f-284f-4093-9f16-fdd0c976797e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>Attendance</th>\n",
       "      <th>...</th>\n",
       "      <th>AT_ShotAccuracy_L5</th>\n",
       "      <th>HT_ShotConversion_L5</th>\n",
       "      <th>AT_ShotConversion_L5</th>\n",
       "      <th>HT_CS_L5</th>\n",
       "      <th>AT_CS_L5</th>\n",
       "      <th>HT_WinRate_L5</th>\n",
       "      <th>AT_WinRate_L5</th>\n",
       "      <th>Ref_Avg_Cards</th>\n",
       "      <th>HT_Last_Win</th>\n",
       "      <th>AT_Last_Win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9465</th>\n",
       "      <td>2025-09-27</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416105</td>\n",
       "      <td>0.147222</td>\n",
       "      <td>0.228099</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.541667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9466</th>\n",
       "      <td>2025-09-27</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.150054</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9467</th>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>Newcastle</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252904</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9468</th>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336084</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.126813</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.419643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>Everton</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455952</td>\n",
       "      <td>0.118586</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.170732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        HomeTeam   AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
       "9465 2025-09-27  Crystal Palace  Liverpool   2.0   1.0   H   1.0   0.0   H   \n",
       "9466 2025-09-27       Tottenham     Wolves   1.0   1.0   D   0.0   0.0   D   \n",
       "9467 2025-09-28       Newcastle    Arsenal   1.0   2.0   A   1.0   0.0   H   \n",
       "9468 2025-09-28     Aston Villa     Fulham   3.0   1.0   H   1.0   1.0   D   \n",
       "9469 2025-09-29         Everton   West Ham   1.0   1.0   D   1.0   0.0   H   \n",
       "\n",
       "      Attendance  ... AT_ShotAccuracy_L5  HT_ShotConversion_L5  \\\n",
       "9465         NaN  ...           0.416105              0.147222   \n",
       "9466         NaN  ...           0.316667              0.150054   \n",
       "9467         NaN  ...           0.252904              0.052500   \n",
       "9468         NaN  ...           0.336084              0.016667   \n",
       "9469         NaN  ...           0.455952              0.118586   \n",
       "\n",
       "      AT_ShotConversion_L5  HT_CS_L5  AT_CS_L5  HT_WinRate_L5  AT_WinRate_L5  \\\n",
       "9465              0.228099       0.6       0.4            0.4            1.0   \n",
       "9466              0.045833       0.6       0.0            0.6            0.0   \n",
       "9467              0.131944       0.8       0.6            0.2            0.6   \n",
       "9468              0.126813       0.4       0.2            0.0            0.4   \n",
       "9469              0.091667       0.4       0.2            0.4            0.2   \n",
       "\n",
       "      Ref_Avg_Cards  HT_Last_Win  AT_Last_Win  \n",
       "9465       3.541667          1.0          1.0  \n",
       "9466       3.812500          0.0          0.0  \n",
       "9467       3.769231          0.0          0.0  \n",
       "9468       3.419643          0.0          1.0  \n",
       "9469       4.170732          0.0          0.0  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53563f-e97a-4c92-b09e-30652142147c",
   "metadata": {},
   "source": [
    "#### 2. Handling sparse & scattered NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7fc116f-c25d-43e9-ab1c-5c8a5b0a2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying columns having NaN values\n",
    "null_columns = {key:value for key, value in dict(df.isnull().sum()).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f70cd-10b3-42a6-9dee-41fcbdb9de8e",
   "metadata": {},
   "source": [
    "STAGE 1: DELETION OF COLUMNS NOT SATISFYING THRESHOLD RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc6e550c-fc99-4947-aab9-19a359950cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Must-Delete columns: {'Attendance': 8711, 'HBP': 8710, 'ABP': 8710, 'B365AH': 8817, 'BFCH': 9090, 'BFCD': 9090, 'BFCA': 9090, 'BFECH': 9030, 'BFECD': 9030, 'BFECA': 9030, 'BFEC>2.5': 9030, 'BFEC<2.5': 9030, 'BFECAHH': 9030, 'BFECAHA': 9030}\n",
      "Length: 14\n"
     ]
    }
   ],
   "source": [
    "# \"Must-Delete\" columns : Columns containing more than 90% NaN values\n",
    "rows = (df.shape)[0]\n",
    "threshold = 0.90\n",
    "must_delete_cols = {key: value for key, value in null_columns.items() if value > (threshold * rows)}\n",
    "print(f\"Must-Delete columns: {must_delete_cols}\")\n",
    "print(f\"Length: {len(must_delete_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07dad169-903e-49a3-8160-0282497970b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after 1st round of deletion: (9470, 121)\n",
      "Length of 'Must-Delete' columns: 14\n",
      "Shape of Original Data frame: (9470, 135)\n"
     ]
    }
   ],
   "source": [
    "df_stage1 = df.drop(columns=list(must_delete_cols.keys()), axis=1)\n",
    "\n",
    "print(f\"Shape after 1st round of deletion: {df_stage1.shape}\")\n",
    "print(f\"Length of 'Must-Delete' columns: {len(must_delete_cols)}\")\n",
    "print(f\"Shape of Original Data frame: {df.shape}\")\n",
    "\n",
    "after_stage1_null_columns = {key:value for key, value in dict(df_stage1.isnull().sum()).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6f1cc-b925-4d45-af8b-74b1f14e4299",
   "metadata": {},
   "source": [
    "STAGE 2: IMPUTING THE NORMALIZED IP COLUMNS (FILLING NULL VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4ed59c6-58ab-47d4-8be1-a66d0e637696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputable columns : Columns containing less than 95% NaN values requires imputing and filling of NaN values\n",
    "imputable_cols = {key: value for key, value in after_stage1_null_columns.items() if value < (threshold * rows) and value != 0}\n",
    "df_stage2 = df_stage1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d1fe285-f533-4662-b607-fe1edd06e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_cols = ['Date', 'HomeTeam', 'AwayTeam', 'Referee', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "exclude_cols = fixed_cols + [col for col in df_stage2.columns if col.startswith(('HT_', 'AT_', 'HG_', 'AG_'))]\n",
    "\n",
    "for col in list(imputable_cols.keys()):\n",
    "    if col not in exclude_cols: df_stage2[col] = df_stage2[col].fillna(df_stage2[col].median())\n",
    "    \n",
    "after_stage2_null_columns = {key:value for key, value in dict(df_stage2.isnull().sum()).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b272c4-3b09-4591-aa14-35b44682747b",
   "metadata": {},
   "source": [
    "STAGE 3: FILLING NULL VALUES OF ROLLING LAG FEATURES AS 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abfa77e7-0512-4abf-865f-29f2ab92b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_cols = {key: value for key, value in after_stage2_null_columns.items() if value > 0}\n",
    "\n",
    "df_stage3 = df_stage2.copy()\n",
    "\n",
    "for col in rolling_cols:\n",
    "    df_stage3[col] = df_stage3[col].fillna(0)\n",
    "    \n",
    "after_stage3_null_columns = {key:value for key, value in dict(df_stage3.isnull().sum()).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280a0c5-ee8b-4ff5-8451-1f5bc3eeb81c",
   "metadata": {},
   "source": [
    "FINAL STAGE: CONVERTING THE RESULTANT DATA FRAME INTO OUR ORIGINAL ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03a43e97-d195-4c9c-88a3-4f6e2a86d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape of the Dataset: (9470, 121)\n"
     ]
    }
   ],
   "source": [
    "df = df_stage3.copy()\n",
    "print(f\"Final Shape of the Dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e776fa08-5749-474a-80fa-8c4718275df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in prob_norm_odds:\n",
    "    df = Probability_Normalization(df, category) \n",
    "for category in prob_only_odds:\n",
    "    df = Probability(df, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b639e90-8c9a-4fc8-bc1e-1f6053c08060",
   "metadata": {},
   "source": [
    "## V. Feature Engineering (II)\n",
    "#### At this step, we add some more derived and engineered features using the existing rolling features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190db03-659e-4e15-b41c-d9fb31129232",
   "metadata": {},
   "source": [
    "#### 1. Ground-Specific Goal Difference (Home Team's strength at Home Ground v/s Away Team's strngth at Away Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de27ed1d-b82a-49ef-81ca-3fede5b51fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GD_Diff_L5'] = df['HG_HT_AvgGD_L5'] - df['AG_AT_AvgGD_L5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909866c4-de90-4899-9d8a-76c34e414cc2",
   "metadata": {},
   "source": [
    "#### 2. Attack v/s Defense (Home Team's Offensive Strength at Home Ground v/s Away Team's Defensive Strength at Away Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82a64ae1-3deb-4a4d-a051-8500644e6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attack_Defense_L5'] = df['HG_HT_AvgGF_L5'] - df['AG_AT_AvgGA_L5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f2aa7-9c38-4252-99fe-79a1ad786365",
   "metadata": {},
   "source": [
    "#### 3. Overall Win Rate Difference (Home Team's Win Rate v/s Away Team's Win Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb0b24ed-4679-46cd-b914-e5b517b26880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Overall_Win_Rate_L5'] = df['HT_WinRate_L5'] - df['AT_WinRate_L5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd99199-3609-4cd7-b3d1-dbf8b37c608a",
   "metadata": {},
   "source": [
    "#### 4. Overall Shot Conversion Difference (Home Team's Shot Conversion Rate v/s Away Team's Shot Conversion Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "840706b0-978a-44e2-96e1-81fbb3d83e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ShotConversion_Diff_L5'] = df['HT_ShotConversion_L5'] - df['AT_ShotConversion_L5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "398bbba4-54c5-4ee4-b731-29ec8c05e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape of the Dataset: (9470, 125)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Shape of the Dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31071c31-83cf-4834-96a0-c33c163b280c",
   "metadata": {},
   "source": [
    "#### 5. Head-to-Head Points (Home Team H2H Points with Away Team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50ec3711-b825-4e5c-8094-070d41f51b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='Date')\n",
    "df['HT_Points'] = np.where(df['FTR'] == 'H', 3, np.where(df['FTR'] == 'D', 1, 0))\n",
    "df['AT_Points'] = np.where(df['FTR'] == 'A', 3, np.where(df['FTR'] == 'D', 1, 0))\n",
    "\n",
    "teams = df[['HomeTeam', 'AwayTeam']].values # creates a 2D numpy array\n",
    "teams.sort(axis=1) # sort the teams alphabetically\n",
    "\n",
    "df['MatchUp'] = teams[:,0] + \"_\" + teams[:,1]\n",
    "df['H2H_HT_Points_L5'] = df.groupby('MatchUp')['HT_Points'].transform(lambda x: x.shift(1).rolling(5, min_periods=1).sum())\n",
    "df['H2H_AT_Points_L5'] = df.groupby('MatchUp')['AT_Points'].transform(lambda x: x.shift(1).rolling(5, min_periods=1).sum())\n",
    "\n",
    "df['H2H_HT_Points_L5'] = df['H2H_HT_Points_L5'].fillna(0)\n",
    "df['H2H_AT_Points_L5'] = df['H2H_AT_Points_L5'].fillna(0)\n",
    "\n",
    "df['H2H_Points_Diff'] = df['H2H_HT_Points_L5'] - df['H2H_AT_Points_L5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4866bdef-6a1f-4935-9fe0-99877745c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['HT_Points', 'AT_Points', 'MatchUp'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "102a16d7-4b6c-473a-8def-05ca69e29323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape of the Dataset: (9470, 128)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Shape of the Dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71908684-20b6-44b5-8a2b-26e80f63871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftwoszn = df.tail(440).reset_index(drop=True).copy()\n",
    "dftwoszn.loc[:379, 'season'] = 2024\n",
    "dftwoszn.loc[380:, 'season'] = 2025\n",
    "dftwoszn['season'] = dftwoszn['season'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "656c82af-6ab3-4f52-adeb-854f4acd9a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 129)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftwoszn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf8559-1218-4888-82b2-25e99f1eba4b",
   "metadata": {},
   "source": [
    "## VI. Working with 24/25 and 25/26 data ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b75034b-dd78-459c-8111-780c095a62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the new datasets have been loaded and ready for merging.\n",
      "Shape of GK stats: (892, 25)\n",
      "Shape of DEF stats: (3378, 41)\n",
      "Shape of MID stats: (3992, 59)\n",
      "Shape of FWD stats: (851, 51)\n",
      "Shape of Teams+Matches stats: (440, 115)\n"
     ]
    }
   ],
   "source": [
    "fe_gk_stats = pd.read_pickle('C:/exp1/Pickle Files/fe_gk_stats.pkl')\n",
    "fe_def_stats = pd.read_pickle('C:/exp1/Pickle Files/fe_def_stats.pkl')\n",
    "fe_mid_stats = pd.read_pickle('C:/exp1/Pickle Files/fe_mid_stats.pkl')\n",
    "fe_fwd_stats = pd.read_pickle('C:/exp1/Pickle Files/fe_fwd_stats.pkl')\n",
    "teams_matches = pd.read_pickle('C:/exp1/Pickle Files/teams_matches.pkl') \n",
    "print(\"All the new datasets have been loaded and ready for merging.\")\n",
    "print(f\"Shape of GK stats: {fe_gk_stats.shape}\")\n",
    "print(f\"Shape of DEF stats: {fe_def_stats.shape}\")\n",
    "print(f\"Shape of MID stats: {fe_mid_stats.shape}\")\n",
    "print(f\"Shape of FWD stats: {fe_fwd_stats.shape}\")\n",
    "print(f\"Shape of Teams+Matches stats: {teams_matches.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f488a0c-7ef7-4cee-842c-5012e2b115a7",
   "metadata": {},
   "source": [
    "### Merging the current dataset with the new data of Teams + Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fadb61-1e0c-4a3c-a0fd-f08527706453",
   "metadata": {},
   "source": [
    "#### Since merge will be done on Date, Home Team and Away Team: It is ensure they are of the same type and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c083af3d-1da1-42a5-9a11-94f27acf36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches['kickoff_time'] = teams_matches['kickoff_time'].dt.normalize()\n",
    "teams_matches = teams_matches.rename(columns={'kickoff_time':'Date'}, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49a1f121-bcb8-4dc0-82e9-6e3d9d6df985",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_matches['HT_name'] = teams_matches['HT_name'].replace({'Man Utd':'Man United', 'Spurs':'Tottenham'})\n",
    "teams_matches['AT_name'] = teams_matches['AT_name'].replace({'Man Utd':'Man United', 'Spurs':'Tottenham'})\n",
    "teams_matches = teams_matches.rename(columns={'HT_name':'HomeTeam', 'AT_name':'AwayTeam'}, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cf5b1-cb04-4f77-805b-69e7bed8fb72",
   "metadata": {},
   "source": [
    "#### It was also found that for season 2025: the dates for the following matches were imputed incorrectly\n",
    "#### Match 1: Brentford vs Aston Villa (23/08/2025) [Incorrect date: 16/09/2025]\n",
    "#### Match 2: Wolves vs Everton (30/08/2025) [Incorrect date: 23/09/2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc01e670-db8d-4778-9d17-c6465c608d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "home1 = 'Brentford'\n",
    "away1 = 'Aston Villa'\n",
    "m1_date = '2025-08-23'\n",
    "m1_faulty = '2025-09-16'\n",
    "\n",
    "home2 = 'Wolves'\n",
    "away2 = 'Everton'\n",
    "m2_date = '2025-08-30'\n",
    "m2_faulty = '2025-09-23'\n",
    "\n",
    "teams_matches.loc[(teams_matches['Date'] == m1_faulty) & (teams_matches['HomeTeam'] == home1), 'Date'] = m1_date\n",
    "teams_matches.loc[(teams_matches['Date'] == m2_faulty) & (teams_matches['HomeTeam'] == home2), 'Date'] = m2_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d5e0566-0612-432f-bbc7-ff3da44ae6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 115)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcfec0d8-0697-4c93-b4e5-452a07b102ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been merged successfully!\n"
     ]
    }
   ],
   "source": [
    "merged_stage1 = dftwoszn.merge(\n",
    "    teams_matches,\n",
    "    on=['Date', 'season', 'HomeTeam', 'AwayTeam'],\n",
    "    how='left',\n",
    "    suffixes=('', '_tm')\n",
    ")\n",
    "merged_stage1 = merged_stage1.drop(columns=[col for col in merged_stage1.columns if col.endswith('_tm')], errors='ignore')\n",
    "print(\"Data has been merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ef1a1ce-27ca-4466-b125-49165eb00220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Merged Data: (440, 240)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of Merged Data: {merged_stage1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f4fb417-98c8-4573-a151-e8cfbb0683e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stats = ['HTHG', 'HTAG', 'HTR', 'Referee', 'home_score', 'away_score', 'gameweek', 'finished', 'match_url',\n",
    "       'home_total_shots', 'away_total_shots', 'home_shots_on_target', 'away_shots_on_target',\n",
    "       'home_fouls_committed', 'away_fouls_committed', 'home_corners', 'away_corners',\n",
    "       'home_yellow_cards', 'away_yellow_cards', 'home_red_cards', 'away_red_cards',\n",
    "       'fotmob_id', 'stats_processed', 'player_stats_processed']\n",
    "\n",
    "odds = [\n",
    "    'GBH', 'GBD', 'GBA', 'BbMxH', 'BbMxD', 'BbMxA',\n",
    "    'GB>2.5', 'GB<2.5', 'B365>2.5', 'B365<2.5', 'BbMx>2.5', 'BbAv>2.5', 'BbMx<2.5', 'BbAv<2.5', 'P>2.5', 'P<2.5',\n",
    "    'B365AHH', 'B365AHA', 'BbAHh', 'BbAvAHH', 'BbAvAHA', 'AHh', 'PAHH', 'PAHA',\n",
    "    'NormIP_BbAvH', 'NormIP_BbAvD', 'NormIP_BbAvA',\n",
    "    'NormIP_MaxH', 'NormIP_MaxD', 'NormIP_MaxA', 'NormIP_AvgH', 'NormIP_AvgD', 'NormIP_AvgA', \n",
    "    'NormIP_B365H', 'NormIP_B365D', 'NormIP_B365A', \n",
    "    'IP_AHO_AvgCAHH', 'IP_AHO_AvgCAHA', 'IP_AHO_PCAHH', 'IP_AHO_PCAHA',\n",
    "]\n",
    "\n",
    "match_stats = [\n",
    "    'home_passes', 'away_passes', 'home_accurate_passes', 'away_accurate_passes', \n",
    "    'home_accurate_passes_pct', 'away_accurate_passes_pct', \n",
    "    'home_shots_off_target', 'away_shots_off_target', \n",
    "    'home_blocked_shots', 'away_blocked_shots', \n",
    "    'home_hit_woodwork', 'away_hit_woodwork',\n",
    "    'home_shots_inside_box', 'away_shots_inside_box', \n",
    "    'home_shots_outside_box', 'away_shots_outside_box',\n",
    "    'home_own_half', 'away_own_half', 'home_opposition_half', 'away_opposition_half',\n",
    "    'home_accurate_long_balls', 'away_accurate_long_balls', 'home_accurate_long_balls_pct', \n",
    "    'away_accurate_long_balls_pct', 'home_accurate_crosses', 'away_accurate_crosses', \n",
    "    'home_accurate_crosses_pct', 'away_accurate_crosses_pct', 'home_throws', 'away_throws',\n",
    "    'home_offsides', 'away_offsides', \n",
    "    'home_tackles_won', 'away_tackles_won', 'home_tackles_won_pct', 'away_tackles_won_pct', \n",
    "    'home_interceptions', 'away_interceptions', 'home_blocks', 'away_blocks', \n",
    "    'home_clearances', 'away_clearances', 'home_keeper_saves', 'away_keeper_saves', \n",
    "    'home_duels_won', 'away_duels_won', 'home_ground_duels_won', 'away_ground_duels_won', \n",
    "    'home_ground_duels_won_pct', 'away_ground_duels_won_pct', 'home_aerial_duels_won', \n",
    "    'away_aerial_duels_won', 'home_aerial_duels_won_pct', 'away_aerial_duels_won_pct', \n",
    "    'home_successful_dribbles', 'away_successful_dribbles', 'home_successful_dribbles_pct', \n",
    "    'away_successful_dribbles_pct',\n",
    "]\n",
    "\n",
    "future_cols_to_drop = raw_stats + match_stats + odds\n",
    "fr_merged_stage1 = merged_stage1.copy()\n",
    "fr_merged_stage1 = fr_merged_stage1.rename(columns={'home_team':'HT_code', 'away_team':'AT_code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc3795-3291-4cc6-b1d1-bbe9a2683b60",
   "metadata": {},
   "source": [
    "### Incorporating & Aggregating Player Match Stats into our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5890795-fcca-4289-abce-e4d694ae0532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                        datetime64[ns]\n",
       "HomeTeam                            object\n",
       "AwayTeam                            object\n",
       "FTHG                               float64\n",
       "FTAG                               float64\n",
       "                                 ...      \n",
       "AT_strength_overall_away             int64\n",
       "AT_strength_attack_away              int64\n",
       "AT_strength_defence_away             int64\n",
       "AT_elo                               int64\n",
       "elo_diff                           float64\n",
       "Length: 240, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_merged_stage1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e22a4b84-8aeb-4514-a2f3-5f116c9b2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating GK stats...\n",
      "GK stats aggregated successfully! Shape: (885, 12)\n",
      "\n",
      "Aggregating DEF stats...\n",
      "DEF stats aggregated successfully! Shape: (1029, 20)\n",
      "\n",
      "Aggregating MID stats...\n",
      "MID stats aggregated successfully! Shape: (1082, 29)\n",
      "\n",
      "Aggregating FWD stats...\n",
      "FWD stats aggregated successfully! Shape: (733, 25)\n",
      "\n",
      "Final Aggregated Players stats ready! Shape: (1255, 80)\n"
     ]
    }
   ],
   "source": [
    "player_stats_df = {\n",
    "    'GK':fe_gk_stats,\n",
    "    'DEF':fe_def_stats,\n",
    "    'MID':fe_mid_stats,\n",
    "    'FWD':fe_fwd_stats\n",
    "}\n",
    "aggregated_data = []\n",
    "for position, stats in player_stats_df.items():\n",
    "    print(f\"Aggregating {position} stats...\")\n",
    "    rolling_cols = [col for col in stats.columns if col.startswith('L5_Avg')]\n",
    "    agg_df = stats.groupby(['match_id', 'team_code'])[rolling_cols].mean().reset_index()\n",
    "    agg_df.columns = ['match_id', 'team_code'] + [f'{position}_{col}' for col in rolling_cols]\n",
    "    aggregated_data.append(agg_df)\n",
    "    print(f\"{position} stats aggregated successfully! Shape: {agg_df.shape}\\n\")\n",
    "final_players_stats = aggregated_data[0]\n",
    "for data in aggregated_data[1:]:\n",
    "    final_players_stats = final_players_stats.merge(\n",
    "        data,\n",
    "        on=['match_id', 'team_code'],\n",
    "        how='outer'\n",
    "    )\n",
    "print(f\"Final Aggregated Players stats ready! Shape: {final_players_stats.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30f9f541-b1a7-46fe-9d8f-e938b870e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_players_stats = final_players_stats.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50dfbabc-000f-4675-9a20-75711815c04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1255, 80)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_players_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f17498-5445-43bd-99d2-e450b7da565d",
   "metadata": {},
   "source": [
    "### Double-Merge (Home & Away) into our Master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0273852-4214-4e70-94f6-32cc62d7eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Team Player stats merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# home team\n",
    "home_final_players_stats = final_players_stats.copy()\n",
    "player_features = [col for col in home_final_players_stats.columns if col not in ['match_id', 'team_code']]\n",
    "home_rename_map = {col: f'HT_TEMP_{col}' for col in player_features}\n",
    "home_final_players_stats = home_final_players_stats.rename(columns=home_rename_map)\n",
    "\n",
    "fr_merged_stage2_step1 = fr_merged_stage1.merge(\n",
    "    home_final_players_stats,\n",
    "    left_on=['match_id', 'HT_code'],\n",
    "    right_on=['match_id', 'team_code'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "home_players = [col for col in fr_merged_stage2_step1.columns if col.startswith('HT_TEMP_')]\n",
    "home_rename_map_final = {col: col.replace('HT_TEMP_', 'HT_') for col in home_players}\n",
    "fr_merged_stage2_step1 = fr_merged_stage2_step1.rename(columns=home_rename_map_final)\n",
    "fr_merged_stage2_step1 = fr_merged_stage2_step1.drop(columns=['team_code'], errors='ignore')\n",
    "print(\"Home Team Player stats merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bf92cf2-740a-4478-82cc-67a01ae710cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Away Team Player stats merged successfully!\n"
     ]
    }
   ],
   "source": [
    "# away team\n",
    "away_final_players_stats = final_players_stats.copy()\n",
    "player_features = [col for col in away_final_players_stats.columns if col not in ['match_id', 'team_code']]\n",
    "away_rename_map = {col: f'AT_TEMP_{col}' for col in player_features}\n",
    "away_final_players_stats = away_final_players_stats.rename(columns=away_rename_map)\n",
    "\n",
    "fr_merged_stage2_step2 = fr_merged_stage2_step1.merge(\n",
    "    away_final_players_stats,\n",
    "    left_on=['match_id', 'AT_code'],\n",
    "    right_on=['match_id', 'team_code'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "away_players = [col for col in fr_merged_stage2_step2.columns if col.startswith('AT_TEMP_')]\n",
    "away_rename_map_final = {col: col.replace('AT_TEMP_', 'AT_') for col in away_players}\n",
    "fr_merged_stage2_step2 = fr_merged_stage2_step2.rename(columns=away_rename_map_final)\n",
    "fr_merged_stage2_step2 = fr_merged_stage2_step2.drop(columns=['team_code'], errors='ignore')\n",
    "print(\"Away Team Player stats merged successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c87d8-d131-4f8a-a71a-928d194db4eb",
   "metadata": {},
   "source": [
    "#### Check for NaN values in the final merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92745b05-2729-433e-a805-d1288ed83554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {key: value for key, value in dict(fr_merged_stage2_step2.isnull().sum()).items() if value > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db39775e-a15a-4db3-9e37-66ff26f2bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has been cleaned completely!\n"
     ]
    }
   ],
   "source": [
    "cols_to_impute = [col for col in fr_merged_stage2_step2.columns if col.startswith(('HT_GK_', 'HT_DEF_', 'HT_MID_', 'HT_FWD_', 'AT_GK_', 'AT_DEF_', 'AT_MID_', 'AT_FWD_'))] \n",
    "player_stats_feature = [col for col in cols_to_impute]\n",
    "fr_merged_stage2_step2[player_stats_feature] = fr_merged_stage2_step2[player_stats_feature].fillna(0)\n",
    "data_analysis = fr_merged_stage2_step2.copy()\n",
    "print(\"The dataset has been cleaned completely!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a731514-d5ca-4f30-94c8-2144e9721645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (master dataframe): (440, 396)\n",
      "Shape (cleaned dataframe): (440, 396)\n"
     ]
    }
   ],
   "source": [
    "final_d1 = data_analysis.copy()\n",
    "test_d1 = fr_merged_stage2_step2.copy()\n",
    "print(f\"Shape (master dataframe): {data_analysis.shape}\")\n",
    "print(f\"Shape (cleaned dataframe): {test_d1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0cdfd0-5c8f-4177-b078-5296a6e5b68c",
   "metadata": {},
   "source": [
    "## VII. Feature Engineering, Feature Reduction and Feature Selection for the Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ca438e9-536a-4ce7-a5aa-09f39fe419d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_stats = [\n",
    "    'HT_strength', 'HT_strength_overall_home', 'HT_strength_attack_home', 'HT_strength_defence_home',\n",
    "    'HT_AvgGF_L5', 'HT_AvgGA_L5', 'HG_HT_AvgGF_L5', 'HG_HT_AvgGA_L5', 'HT_AvgShots_L5',\n",
    "    'HT_ShotAccuracy_L5', 'HT_ShotConversion_L5', 'HT_CS_L5', 'HT_WinRate_L5'\n",
    "]\n",
    "players_stats = [\n",
    "    col.replace('HT_', '').replace('AT_', '')\n",
    "    for col in test_d1.columns \n",
    "    if col.startswith('HT_GK') or col.startswith('HT_DEF') or col.startswith('HT_MID') or col.startswith('HT_FWD')\n",
    "]\n",
    "elo_features = ['ht_match_elo', 'at_match_elo']\n",
    "match_features = ['home_possession', 'away_possession', 'home_expected_goals_xg', 'away_expected_goals_xg',\n",
    "                  'home_big_chances', 'away_big_chances', 'home_big_chances_missed', 'away_big_chances_missed',\n",
    "                  'home_xg_open_play', 'away_xg_open_play', 'home_xg_set_play', 'away_xg_set_play', \n",
    "                  'home_non_penalty_xg', 'away_non_penalty_xg', 'home_xg_on_target_xgot', 'away_xg_on_target_xgot',\n",
    "                  'home_touches_in_opposition_box', 'away_touches_in_opposition_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ca771e6-b177-46c4-b9f0-13aaa4dc509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering and Feature Reduction done successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT0001\\AppData\\Local\\Temp\\ipykernel_26420\\449208135.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_d1[new_col] = test_d1[home_col] - test_d1[away_col]\n"
     ]
    }
   ],
   "source": [
    "player_base_features = [col.replace('HT_', '') for col in test_d1.columns \n",
    "                        if col.startswith('HT_GK') or col.startswith('HT_DEF') or col.startswith('HT_MID') or col.startswith('HT_FWD')]\n",
    "player_base_features = sorted(list(set(player_base_features)))\n",
    "\n",
    "original_cols_to_drop = []\n",
    "\n",
    "for home_col in teams_stats:\n",
    "    base_col = home_col.replace('HT_', '')\n",
    "    away_col = f'AT_{base_col}'\n",
    "    if away_col in test_d1.columns:\n",
    "        test_d1[f'{base_col}_Diff'] = test_d1[home_col] - test_d1[away_col]\n",
    "        original_cols_to_drop.extend([home_col, away_col])\n",
    "\n",
    "for features in player_base_features:\n",
    "    home = f'HT_{features}'\n",
    "    away = f'AT_{features}'\n",
    "    test_d1[f'{features}_Diff'] = test_d1[home] - test_d1[away]\n",
    "    original_cols_to_drop.extend([home, away])\n",
    "\n",
    "original_cols_to_drop.extend(elo_features)\n",
    "\n",
    "for idx in range(0, len(match_features), 2):\n",
    "    home_col = match_features[idx]\n",
    "    away_col = match_features[idx+1]\n",
    "    new_col = f'{home_col.replace('home_', '').replace('_xg', '')}_Diff'\n",
    "    test_d1[new_col] = test_d1[home_col] - test_d1[away_col]\n",
    "    original_cols_to_drop.extend([home_col, away_col])\n",
    "\n",
    "original_cols_to_drop = list(set(original_cols_to_drop))\n",
    "\n",
    "future_cols_to_drop_set = set(future_cols_to_drop)\n",
    "original_cols_to_drop_set = set(original_cols_to_drop)\n",
    "union_cols_to_drop_set = future_cols_to_drop_set.union(original_cols_to_drop_set)\n",
    "union_cols_to_drop = list(union_cols_to_drop_set)\n",
    "\n",
    "data_analysis = test_d1.copy()\n",
    "test_d2 = test_d1.drop(columns=union_cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"Feature Engineering and Feature Reduction done successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "553be419-8e54-4806-bb41-77533dcd8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of master dataframe (after): (440, 491)\n",
      "Shape of cleaned dataframe (after): (440, 178)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of master dataframe (after): {data_analysis.shape}\")\n",
    "print(f\"Shape of cleaned dataframe (after): {test_d2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb9620-ccf2-4d4c-a989-26983ccba44a",
   "metadata": {},
   "source": [
    "## VIII. Data Preparation for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8ba76-ad79-4d0a-a3c8-365d74a9ccd6",
   "metadata": {},
   "source": [
    "#### For Classification: Target Feature = 'FTR' (Full Time Result) - This must be encoded into numerical values, using Label Encoding\n",
    "#### Along with it, 'HTR' (Half Time Result) should also be encoded in a similar fashion since it poses high predictive strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78b78565-07b8-4ec3-abd9-23439b523a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_d2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3fcdbf8b-6a8f-426f-a92e-b5996817b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape of the Dataset (before): (440, 178)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Shape of the Dataset (before): {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f46f2cb-d4ac-4470-a692-82391a43203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape of the dataset (after): (440, 138)\n"
     ]
    }
   ],
   "source": [
    "df_removal = df.copy()\n",
    "output_target = df_removal['FTR']\n",
    "output_regression_home = df_removal['FTHG']\n",
    "output_regression_away = df_removal['FTAG']\n",
    "dropped_cols = [\n",
    "    'FTHG', 'FTAG',\n",
    "    'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR',\n",
    "    'Date', 'season', 'match_id', 'MatchUp',\n",
    "    'HomeTeam', 'AwayTeam', 'HT_Last_Win', 'AT_Last_Win',\n",
    "    'HT_code', 'AT_code', 'HT_Points', 'AT_Points',\n",
    "    'H2H_HT_Points_L5', 'H2H_AT_Points_L5',\n",
    "    'HT_elo', 'AT_elo',\n",
    "    'HT_strength_overall_home', 'AT_strength_overall_away',\n",
    "    'HT_strength_attack_home', 'AT_strength_attack_away',\n",
    "    'HT_strength_defence_home', 'AT_strength_defence_away',\n",
    "    'HG_HT_AvgGF_L5', 'HG_HT_AvgGA_L5', 'HG_HT_AvgGD_L5', 'AG_AT_AvgGF_L5', 'AG_AT_AvgGA_L5', 'AG_AT_AvgGD_L5',\n",
    "    'HT_Encoded_Strength', 'AT_Encoded_Strength', \n",
    "    'GK_L5_Avg_team_goals_conceded_Diff' \n",
    "]\n",
    "df_removal = df_removal.drop(columns=dropped_cols, errors='ignore')\n",
    "print(f\"Final Shape of the dataset (after): {df_removal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "536aaea6-22d7-4656-9621-fc37321aabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into Input and Output features\n"
     ]
    }
   ],
   "source": [
    "X_c1 = df_removal.drop(columns='FTR', errors='ignore').copy()\n",
    "y_c1 = pd.get_dummies(output_target, prefix='Result')\n",
    "print(\"Data successfully split into Input and Output features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71789a-ee50-488f-a08b-2de5a7765b09",
   "metadata": {},
   "source": [
    "## IX. Train-Test Splitting & Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec311c-809b-4509-b818-9f17b0c3f892",
   "metadata": {},
   "source": [
    "### Train & Test: Chronological Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ee31d6e-4375-4c3f-97ce-95314788985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification data successfully split chronologically into training & testing sets\n",
      "Shape of Training Set: (330, 137)\n",
      "Shape of Test Set: (110, 137)\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE_RATIO = 0.25\n",
    "split_point = int(len(X_c1) * (1 - TEST_SIZE_RATIO))\n",
    "\n",
    "X_train_c1 = X_c1.iloc[:split_point].copy()\n",
    "X_test_c1 = X_c1.iloc[split_point:].copy()\n",
    "\n",
    "y_train_c1 = y_c1.iloc[:split_point].copy()\n",
    "y_test_c1 = y_c1.iloc[split_point:].copy()\n",
    "\n",
    "y_train_r_home = output_regression_home.iloc[:split_point].copy()\n",
    "y_test_r_home = output_regression_home.iloc[split_point:].copy()\n",
    "\n",
    "y_train_r_away = output_regression_away.iloc[:split_point].copy()\n",
    "y_test_r_away = output_regression_away.iloc[split_point:].copy()\n",
    "\n",
    "print(\"Classification data successfully split chronologically into training & testing sets\")\n",
    "print(f\"Shape of Training Set: {X_train_c1.shape}\")\n",
    "print(f\"Shape of Test Set: {X_test_c1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd9724-cab4-4fe8-bd90-f6e94abf5d89",
   "metadata": {},
   "source": [
    "### Scaling the Non-Target numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "614e7a08-b65f-4d4c-9162-fdbd98f02b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features have been scaled correctly (fit on train, transform on test)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_c1[X_train_c1.columns] = scaler.fit_transform(X_train_c1[X_train_c1.columns])\n",
    "X_test_c1[X_test_c1.columns] = scaler.transform(X_test_c1[X_test_c1.columns])\n",
    "print(\"All features have been scaled correctly (fit on train, transform on test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39b5e2-b4bd-4c44-8b10-e40562575ce8",
   "metadata": {},
   "source": [
    "### Renaming the columns for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c86322d-3ae0-4520-ade4-fa0d514c1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c1.columns = X_train_c1.columns.str.replace('>', '_GT_', regex=False).str.replace('<', '_LT_', regex=False).str.replace('.', '_', regex=False)\n",
    "X_test_c1.columns = X_test_c1.columns.str.replace('>', '_GT_', regex=False).str.replace('<', '_LT_', regex=False).str.replace('.', '_', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7790b8-bdf1-40be-b858-3f55f6a5a118",
   "metadata": {},
   "source": [
    "### Label Encoding: For encoding output feature, Establishing class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b83bf926-a364-47d0-bfa1-c679a7cce4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = y_train_c1.idxmax(axis=1)\n",
    "y_test_labels = y_test_c1.idxmax(axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([y_train_labels, y_test_labels]).unique()) \n",
    "y_train_encoded = le.transform(y_train_labels)\n",
    "y_test_encoded = le.transform(y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be1198ac-2be4-4cde-9ba1-51d8726541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train_encoded)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=classes, \n",
    "    y=y_train_encoded \n",
    ")\n",
    "sample_weight = np.array([class_weights[i] for i in y_train_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9d31cc9-9dea-4959-b0a8-bbc06273c37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 137)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6e96e47-b570-4fbe-a8ca-b98e1db26116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 137)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_c1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7d7eb-acb8-4d9e-ab85-f9ca12ef581e",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f553f6ee-a5c5-4066-8433-f622f465e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the XGBoost Ensemble Model...\n",
      "Training the model now...\n",
      "Predicting on test data...\n",
      "Evaluating the performance of the model...\n",
      "ACCURACY (Fine-Tuned): 71.8182%\n",
      "CLASSIFICATION REPORT (Fine-Tuned):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.82      0.73        33\n",
      "           1       0.59      0.36      0.44        28\n",
      "           2       0.81      0.86      0.83        49\n",
      "\n",
      "    accuracy                           0.72       110\n",
      "   macro avg       0.68      0.68      0.67       110\n",
      "weighted avg       0.71      0.72      0.70       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the XGBoost Ensemble Model...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.09,\n",
    "    num_class=3, \n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Training the model now...\")\n",
    "xgb_model.fit(X_train_c1, y_train_encoded, sample_weight=sample_weight)\n",
    "print(\"Predicting on test data...\")\n",
    "prediction = xgb_model.predict(X_test_c1)\n",
    "print(\"Evaluating the performance of the model...\")\n",
    "accuracy = accuracy_score(y_test_encoded, prediction) \n",
    "print(f\"ACCURACY (Fine-Tuned): {accuracy * 100:.4f}%\")\n",
    "report = classification_report(y_test_encoded, prediction)\n",
    "print(\"CLASSIFICATION REPORT (Fine-Tuned):\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26218fc5-b0b1-4b70-bce8-c6e79d3425e5",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b64c3748-3e78-4ac9-8b73-a400cc64b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_c1.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "imp_features = importance_df['Feature'].to_list()\n",
    "X_train_1 = X_train_c1[imp_features].copy()\n",
    "X_test_1 = X_test_c1[imp_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e236ac-fddd-4a92-89b7-6d15ee524e54",
   "metadata": {},
   "source": [
    "### Stratified K-Fold and Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bff23fb0-64b5-4457-93b8-97f04b22adee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'max_depth': [3, 4, 5], \n",
    "#     'learning_rate': [0.01, 0.05, 0.09], \n",
    "#     'min_child_weight': [0.5, 1, 2], \n",
    "#     'gamma': [0.5, 1], \n",
    "# }\n",
    "\n",
    "# xgb_clf = xgb.XGBClassifier(\n",
    "#     objective='multi:softmax',\n",
    "#     n_estimators=1000, \n",
    "#     num_class=3,\n",
    "#     eval_metric='mlogloss',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb_clf,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='f1_macro', \n",
    "#     cv=skf,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1 \n",
    "# )\n",
    "\n",
    "# print(\"Starting Grid Search on top 60 features...\")\n",
    "# grid_search.fit(X_train_1, y_train_encoded, sample_weight=sw)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(\"\\nBest Parameters found:\", grid_search.best_params_)\n",
    "# print(\"Best Cross-Validation Macro F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0343909-b375-4f9d-bf93-b83ea1e76dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model now...\n",
      "Predicting on test data...\n",
      "\n",
      "BEST ACCURACY (Optimized Model): 67.2727%\n",
      "BEST CLASSIFICATION REPORT (Optimized Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69        33\n",
      "           1       0.43      0.43      0.43        28\n",
      "           2       0.81      0.80      0.80        49\n",
      "\n",
      "    accuracy                           0.67       110\n",
      "   macro avg       0.64      0.64      0.64       110\n",
      "weighted avg       0.67      0.67      0.67       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params = {'gamma': 1, 'learning_rate': 0.09, 'max_depth': 4, 'min_child_weight': 2}\n",
    "final_xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    n_estimators=1000,\n",
    "    num_class=3,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42, \n",
    "    **best_params\n",
    ")\n",
    "print(\"Training the model now...\")\n",
    "final_xgb_model.fit(X_train_1, y_train_encoded, sample_weight=sample_weight) \n",
    "print(\"Predicting on test data...\")\n",
    "best_prediction = final_xgb_model.predict(X_test_1)\n",
    "best_accuracy = accuracy_score(y_test_encoded, best_prediction)\n",
    "best_report = classification_report(y_test_encoded, best_prediction)\n",
    "print(f\"\\nBEST ACCURACY (Optimized Model): {best_accuracy * 100:.4f}%\")\n",
    "print(\"BEST CLASSIFICATION REPORT (Optimized Model):\")\n",
    "print(best_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0f0ac-1c6a-4dfd-85f1-0b43e4fb4e2a",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf175f24-a5c5-4bff-bd9a-ed5ff23636de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Home Goals Regressor...\n",
      "Starting training for Away Goals Regressor...\n",
      "\n",
      "REGRESSION MODEL PERFORMANCE (Score Prediction)\n",
      "Home Goals (FTHG) Metrics:\n",
      "Mean Absolute Error (MAE): 0.7545\n",
      "Mean Squared Error (MSE):  0.9909\n",
      "\n",
      "Away Goals (FTAG) Metrics:\n",
      "Mean Absolute Error (MAE): 0.7545\n",
      "Mean Squared Error (MSE):  0.9545\n",
      "\n",
      "Overall Scoreline Health\n",
      "Average MAE (Scoreline): 0.7545\n"
     ]
    }
   ],
   "source": [
    "rf_home_goals = RandomForestRegressor(\n",
    "    n_estimators=1000,     \n",
    "    max_depth=10,         \n",
    "    random_state=42,\n",
    "    n_jobs=-1             \n",
    ")\n",
    "rf_away_goals = RandomForestRegressor(\n",
    "    n_estimators=1000,     \n",
    "    max_depth=10,         \n",
    "    random_state=42,\n",
    "    n_jobs=-1             \n",
    ")\n",
    "\n",
    "print(\"Starting training for Home Goals Regressor...\")\n",
    "rf_home_goals.fit(X_train_c1, y_train_r_home)\n",
    "print(\"Starting training for Away Goals Regressor...\")\n",
    "rf_away_goals.fit(X_train_c1, y_train_r_away)\n",
    "\n",
    "pred_home_goals_float = rf_home_goals.predict(X_test_c1)\n",
    "pred_away_goals_float = rf_away_goals.predict(X_test_c1)\n",
    "\n",
    "pred_home_goals_int = np.round(np.maximum(0, pred_home_goals_float)).astype(int)\n",
    "pred_away_goals_int = np.round(np.maximum(0, pred_away_goals_float)).astype(int)\n",
    "\n",
    "mae_home = mean_absolute_error(y_test_r_home, pred_home_goals_int)\n",
    "mae_away = mean_absolute_error(y_test_r_away, pred_away_goals_int)\n",
    "\n",
    "mse_home = mean_squared_error(y_test_r_home, pred_home_goals_int)\n",
    "mse_away = mean_squared_error(y_test_r_away, pred_away_goals_int)\n",
    "\n",
    "print(\"\\nREGRESSION MODEL PERFORMANCE (Score Prediction)\")\n",
    "\n",
    "# Display Home Goal Metrics\n",
    "print(\"Home Goals (FTHG) Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_home:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE):  {mse_home:.4f}\")\n",
    "\n",
    "# Display Away Goal Metrics\n",
    "print(\"\\nAway Goals (FTAG) Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_away:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE):  {mse_away:.4f}\")\n",
    "\n",
    "# Overall Model Health\n",
    "print(\"\\nOverall Scoreline Health\")\n",
    "print(f\"Average MAE (Scoreline): {(mae_home + mae_away) / 2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36f2c9c9-ce3a-4c25-ad26-a8e8f82be748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Score: 1.0 - 0.0 | Predicted Score: 2 - 1\n",
      "Actual Score: 1.0 - 0.0 | Predicted Score: 2 - 1\n",
      "Actual Score: 2.0 - 2.0 | Predicted Score: 1 - 1\n",
      "Actual Score: 2.0 - 0.0 | Predicted Score: 2 - 1\n",
      "Actual Score: 1.0 - 2.0 | Predicted Score: 1 - 1\n",
      "Actual Score: 4.0 - 3.0 | Predicted Score: 2 - 1\n",
      "Actual Score: 1.0 - 1.0 | Predicted Score: 1 - 2\n",
      "Actual Score: 1.0 - 1.0 | Predicted Score: 1 - 1\n",
      "Actual Score: 3.0 - 1.0 | Predicted Score: 2 - 1\n",
      "Actual Score: 1.0 - 1.0 | Predicted Score: 1 - 1\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10, 20):\n",
    "    print(f\"Actual Score: {y_test_r_home.iloc[idx]} - {y_test_r_away.iloc[idx]} | Predicted Score: {pred_home_goals_int[idx]} - {pred_away_goals_int[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f67cea29-8aec-4836-bdd7-dffd30fee2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bdb24-57c5-4df1-ba6d-bf4cd67a15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load(r\"C:\\PROJECT\\data_artifacts\\master_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2766f0e0-171d-4cd0-b49d-fdb33394b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='Date', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a1b3c485-9956-45e7-9de3-abaedb68fbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>FTR</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HTAG</th>\n",
       "      <th>HTR</th>\n",
       "      <th>Referee</th>\n",
       "      <th>...</th>\n",
       "      <th>MID_L5_Avg_xg_Diff</th>\n",
       "      <th>possession_Diff</th>\n",
       "      <th>expected_goals_Diff</th>\n",
       "      <th>big_chances_Diff</th>\n",
       "      <th>big_chances_missed_Diff</th>\n",
       "      <th>xg_open_play_Diff</th>\n",
       "      <th>xg_set_play_Diff</th>\n",
       "      <th>non_penalty_Diff</th>\n",
       "      <th>xg_on_targetot_Diff</th>\n",
       "      <th>touches_in_opposition_box_Diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>2024-11-09</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H</td>\n",
       "      <td>D Coote</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date   HomeTeam     AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
       "335 2024-11-09  Liverpool  Aston Villa   2.0   0.0   H   1.0   0.0   H   \n",
       "\n",
       "     Referee  ...  MID_L5_Avg_xg_Diff  possession_Diff  expected_goals_Diff  \\\n",
       "335  D Coote  ...              0.0299             24.0                 0.64   \n",
       "\n",
       "     big_chances_Diff  big_chances_missed_Diff  xg_open_play_Diff  \\\n",
       "335              -1.0                     -3.0               1.34   \n",
       "\n",
       "     xg_set_play_Diff  non_penalty_Diff  xg_on_targetot_Diff  \\\n",
       "335             -0.69              0.64                 0.64   \n",
       "\n",
       "     touches_in_opposition_box_Diff  \n",
       "335                             2.0  \n",
       "\n",
       "[1 rows x 491 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['HomeTeam'] == 'Liverpool') & (df['AwayTeam'] == 'Aston Villa')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85f43131-f4da-456a-bad3-3b9db2c67e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[(df['HomeTeam'] == 'Liverpool') & (df['AwayTeam'] == 'Brentford')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117dd84-8b72-4f48-af39-d767e5d78edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ee767ca-4dec-4adf-b1cf-5ff2f39b592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = joblib.load(r\"C:\\PROJECT\\data_artifacts\\final_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886398b-c914-4903-9e48-ae4c1e08be51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f66b1f-de37-40af-a6ad-d97b652e03e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final venv311",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
